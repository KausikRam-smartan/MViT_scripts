{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8157293b",
   "metadata": {},
   "source": [
    "# MVit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765e27e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "_transforms_video is available\n",
      "Fallback _transforms_video not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "from config import *\n",
    "import os, time\n",
    "\n",
    "from decord import VideoReader, cpu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b93ac8",
   "metadata": {},
   "source": [
    "The MViT Requirement\n",
    "\n",
    "The MViT model is a 3D video network and requires its input tensor to be ordered as:\n",
    "(Channels,Time,Height,Width)or(C,T,H,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1680ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        print(\"########### BUILD INDEX TRACKING ###########\")\n",
    "        classes = sorted([d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))])\n",
    "        print(f\" |Classes: {classes}\")\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        print(f\" |Class_to_idx: {self.class_to_idx}\")\n",
    "        for cls_name in classes:\n",
    "            cls_dir = os.path.join(self.root_dir, cls_name)\n",
    "            print(f\" |Class_directory: {cls_dir}\")\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith(\".mp4\"):\n",
    "                    self.video_paths.append(os.path.join(cls_dir, fname))\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "        print(f\" |Num videos: {len(self.video_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            # Frame indices\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                # pad by repeating last frame (simple + robust)\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()          # (T,H,W,C)\n",
    "\n",
    "            # Fix for grayscale videos\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   # (C,T,H,W)\n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)                                    # keep (C,T,H,W)\n",
    "\n",
    "            return frames, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            # try next video (avoid infinite recursion if dataset has 0 length)\n",
    "            return self.__getitem__((idx + 1) % len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca9fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Get one batch\n",
    "# for video_batch, label_batch in train_loader:\n",
    "#     print(f\"Shape of one video batch: {video_batch.shape}\")\n",
    "\n",
    "#     break # Stop after the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97cc0fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready for fine-tuning with 30 output classes.\n",
      "Activating Optimizer Adam\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'YourVideoDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Datasets and DataLoader Setup\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mYourVideoDataset\u001b[49m(train_datapath, transform\u001b[38;5;241m=\u001b[39mtransform, frames_per_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     65\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m YourVideoDataset(val_datapath, transform\u001b[38;5;241m=\u001b[39mtransform, frames_per_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     67\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m(device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YourVideoDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load the Pretrained Model\n",
    "# =========================\n",
    "\n",
    "# 1. Define the desired weights\n",
    "weights = MViT_V1_B_Weights.DEFAULT  # or KINETICS400_V1 depending on your task\n",
    "\n",
    "# 2. Load the model with the pretrained weights\n",
    "model = mvit_v1_b(weights=weights)\n",
    "\n",
    "# =========================\n",
    "# Freezing Layers and Modifying the Head\n",
    "# =========================\n",
    "\n",
    "# --- FREEZE ALL LAYERS IN THE BACKBONE ---\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# The last layer in the `head` will be a Linear layer, so we access it as follows:\n",
    "last_fc_layer = model.head[-1]  # Get the last layer of the head (should be a Linear layer)\n",
    "in_features = last_fc_layer.in_features  # Get the number of input features to this layer\n",
    "\n",
    "# Replace the last Linear layer in `head` with a new one that has the number of output classes you need\n",
    "num_classes = 30  # Replace with your own number of classes\n",
    "model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "print(f\"Model ready for fine-tuning with {num_classes} output classes.\")\n",
    "\n",
    "# Unfreezing few layers\n",
    "K =  3\n",
    "blocks = list(model.blocks)\n",
    "\n",
    "for block in blocks[-K:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad =True\n",
    "\n",
    "# =========================\n",
    "# Optimizer\n",
    "# =========================\n",
    "\n",
    "optimizer_name = \"Adam\"  # This should be defined before or passed as an argument\n",
    "learning_rate = 1e-4  # Set learning rate as per your requirement\n",
    "\n",
    "if optimizer_name == \"Adam\":\n",
    "    print(f\"Activating Optimizer {optimizer_name}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "# =========================\n",
    "# Datasets and DataLoader Setup\n",
    "# =========================\n",
    "\n",
    "train_dataset = YourVideoDataset(train_datapath, transform=transform, frames_per_clip=16)\n",
    "val_dataset = YourVideoDataset(val_datapath, transform=transform, frames_per_clip=16)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\" Counts the total number of trainable parameters in a PyTorch model. \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "total_trainable_params = count_trainable_parameters(model)\n",
    "print(f\"\\nTotal trainable parameters in the model: {total_trainable_params:,}\")\n",
    "\n",
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (cross entropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "best_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize variables to track metrics\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "\n",
    "    # Iterate through the training dataset\n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        # Move inputs and targets to the correct device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients for each batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradient norms to avoid zero/NaN gradients\n",
    "        any_finite = False\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                g = p.grad.detach()\n",
    "                gnorm = g.data.norm(2).item()\n",
    "                if np.isfinite(gnorm) and gnorm > 0:\n",
    "                    any_finite = True\n",
    "\n",
    "        if not any_finite:\n",
    "            print(\"[grad check] All parameter grad norms are zero/NaN this step.\")\n",
    "\n",
    "        # Compute total gradient norm (to monitor exploding/vanishing gradients)\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                pn = p.grad.data.norm(2).item()\n",
    "                total_norm_sq += pn * pn\n",
    "        total_grad_norm = float(total_norm_sq ** 0.5)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics for loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        # Collect epoch training metrics\n",
    "        preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        all_train_preds.extend(preds.tolist())\n",
    "        all_train_labels.extend(targets.detach().cpu().numpy().tolist())\n",
    "        \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples * 100\n",
    "\n",
    "    # Epoch metrics\n",
    "    avg_train_loss = running_loss / max(1, len(train_loader))\n",
    "    train_accuracy = accuracy_score(all_train_labels, all_train_preds) if all_train_labels else 0.0\n",
    "    train_precision = precision_score(all_train_labels, all_train_preds, average='weighted', zero_division=0) if all_train_labels else 0.0\n",
    "    train_recall    = recall_score(all_train_labels, all_train_preds,   average='weighted', zero_division=0) if all_train_labels else 0.0\n",
    "    train_f1        = f1_score(all_train_labels, all_train_preds,       average='weighted', zero_division=0) if all_train_labels else 0.0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Training Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  Training Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # =========================\n",
    "    # Validation Phase\n",
    "    # =========================\n",
    "    \n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation for validation (saves memory)\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        all_val_preds = [] \n",
    "        all_val_targets = []\n",
    "        \n",
    "        # Iterate through the validation dataset\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move inputs and targets to the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "            all_val_preds.extend(predicted.detach().cpu().numpy())\n",
    "            all_val_targets.extend(targets.detach().cpu().numpy())\n",
    "        \n",
    "        # Calculate average loss and accuracy for validation\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct_predictions / total_samples * 100\n",
    "\n",
    "        eval_accuracy = correct_predictions / total_samples if total_samples else 0.0\n",
    "        eval_precision = precision_score(all_val_targets, all_val_preds, average='macro', zero_division=0)\n",
    "        eval_recall    = recall_score(all_val_targets, all_val_preds, average='macro', zero_division=0)\n",
    "        eval_f1        = f1_score(all_val_targets, all_val_preds, average='macro', zero_division=0)\n",
    "        \n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    saved= False\n",
    "    # Checkpoint Logic\n",
    "    if val_loss < best_loss:\n",
    "        print(f\"  *** Validation Loss improved from {best_loss:.4f} to {val_loss:.4f}. Saving model. ***\")\n",
    "        best_loss = val_loss\n",
    "        \n",
    "        # Save only the model's parameters (state_dict)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        torch.save(model, model_save_arch_path)\n",
    "        saved= True\n",
    "    else:\n",
    "        print(f\"  Validation Loss did not improve.\")\n",
    "\n",
    "    # Logs\n",
    "    print(f\"Train Loss: {avg_train_loss:.6f}, Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"Eval  Loss: {val_loss:.6f}, Acc: {eval_accuracy:.4f}, Prec: {eval_precision:.4f}, Rec: {eval_recall:.4f}, F1: {eval_f1:.4f}\")\n",
    "    print(f\"Grad Norm (last step): {total_grad_norm:.6f}, LR: {optimizer.param_groups[0]['lr']:.10f}\")\n",
    "    if saved:\n",
    "        print(f\"Model saved to: {os.path.abspath(model_save_path)}\")\n",
    "    print(f\"Epoch Time Taken: {(time.time() - epoch_start_time):.2f} sec\")\n",
    "\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch+1}/{num_epochs}\\n\")\n",
    "        f.write(f\"Train Loss: {avg_train_loss:.6f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\\n\")\n",
    "        f.write(f\"Eval Loss: {val_loss:.6f}, Accuracy: {eval_accuracy:.4f}, Precision: {eval_precision:.4f}, Recall: {eval_recall:.4f}, F1: {eval_f1:.4f}\\n\")\n",
    "        f.write(f\"Grad Norm: {total_grad_norm:.6f}, Learning Rate: {optimizer.param_groups[0]['lr']:.10f}\\n\")\n",
    "        if saved:\n",
    "            f.write(f\"Model saved to: {os.path.abspath(model_save_path)}\\n\")\n",
    "        f.write(f\"Epoch Time Taken: {(time.time() - epoch_start_time):.2f} sec\\n\")\n",
    "        f.write(\"----------------------------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ddeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "cuda device count: 1\n",
      "current device: 0\n",
      "device name: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda device count:\", torch.cuda.device_count())\n",
    "    print(\"current device:\", torch.cuda.current_device())\n",
    "    print(\"device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833a3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "_transforms_video is available\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Fallback _transforms_video not available\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "########### BUILD INDEX TRACKING ###########\n",
      " |Classes: ['arm_circles', 'bb_military_press', 'bb_upright', 'bicep_curls', 'db_chest_press', 'db_incline_chest_press', 'db_lunges', 'db_reverse_flys', 'db_seated_shoulder_press', 'ez_bb_curls', 'front_raise', 'hammer_curls', 'kb_goblet_squats', 'kb_goodmorning', 'kb_overhead_press', 'kb_swings', 'lateral_raise', 'skull_crushers', 'spider_curls', 'tricep_dips', 'zottman_curls']\n",
      " |Class_to_idx: {'arm_circles': 0, 'bb_military_press': 1, 'bb_upright': 2, 'bicep_curls': 3, 'db_chest_press': 4, 'db_incline_chest_press': 5, 'db_lunges': 6, 'db_reverse_flys': 7, 'db_seated_shoulder_press': 8, 'ez_bb_curls': 9, 'front_raise': 10, 'hammer_curls': 11, 'kb_goblet_squats': 12, 'kb_goodmorning': 13, 'kb_overhead_press': 14, 'kb_swings': 15, 'lateral_raise': 16, 'skull_crushers': 17, 'spider_curls': 18, 'tricep_dips': 19, 'zottman_curls': 20}\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/arm_circles\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/bb_military_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/bb_upright\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/bicep_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/db_chest_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/db_incline_chest_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/db_lunges\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/db_reverse_flys\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/db_seated_shoulder_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/ez_bb_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/front_raise\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/hammer_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/kb_goblet_squats\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/kb_goodmorning\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/kb_overhead_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/kb_swings\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/lateral_raise\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/skull_crushers\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/spider_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/tricep_dips\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/train/zottman_curls\n",
      " |Num videos: 1664\n",
      "########### BUILD INDEX TRACKING ###########\n",
      " |Classes: ['arm_circles', 'bb_military_press', 'bb_upright', 'bicep_curls', 'db_chest_press', 'db_incline_chest_press', 'db_lunges', 'db_reverse_flys', 'db_seated_shoulder_press', 'ez_bb_curls', 'front_raise', 'hammer_curls', 'kb_goblet_squats', 'kb_goodmorning', 'kb_overhead_press', 'kb_swings', 'lateral_raise', 'skull_crushers', 'spider_curls', 'tricep_dips', 'zottman_curls']\n",
      " |Class_to_idx: {'arm_circles': 0, 'bb_military_press': 1, 'bb_upright': 2, 'bicep_curls': 3, 'db_chest_press': 4, 'db_incline_chest_press': 5, 'db_lunges': 6, 'db_reverse_flys': 7, 'db_seated_shoulder_press': 8, 'ez_bb_curls': 9, 'front_raise': 10, 'hammer_curls': 11, 'kb_goblet_squats': 12, 'kb_goodmorning': 13, 'kb_overhead_press': 14, 'kb_swings': 15, 'lateral_raise': 16, 'skull_crushers': 17, 'spider_curls': 18, 'tricep_dips': 19, 'zottman_curls': 20}\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/arm_circles\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/bb_military_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/bb_upright\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/bicep_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/db_chest_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/db_incline_chest_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/db_lunges\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/db_reverse_flys\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/db_seated_shoulder_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/ez_bb_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/front_raise\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/hammer_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/kb_goblet_squats\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/kb_goodmorning\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/kb_overhead_press\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/kb_swings\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/lateral_raise\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/skull_crushers\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/spider_curls\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/tricep_dips\n",
      " |Class_directory: /home/smartan5070/Downloads/SlowfastTrainer-main/dataset_21_class/21_class_12_12_25_script_balanced_80_15/val/zottman_curls\n",
      " |Num videos: 316\n",
      "Model ready for fine-tuning with 21 output classes.\n",
      "Activating Optimizer Adam\n",
      "\n",
      "Total trainable parameters in the model: 22,211,829\n",
      "Epoch 1/15: 100%|█████████████████████████████| 416/416 [01:09<00:00,  5.96it/s]\n",
      "Epoch 1/15:\n",
      "  Training Loss: 1.5549\n",
      "  Training Accuracy: 70.01%\n",
      "  Validation Loss: 0.4202\n",
      "  Validation Accuracy: 93.99%\n",
      "  *** Validation Loss improved from inf to 0.4202. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 1.554921, Acc: 0.7001, Prec: 0.7249, Rec: 0.7001, F1: 0.7034\n",
      "Eval  Loss: 0.420249, Acc: 0.9399, Prec: 0.9583, Rec: 0.9397, F1: 0.9258\n",
      "Grad Norm (last step): 4.851160, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 80.06 sec\n",
      "Epoch 2/15: 100%|█████████████████████████████| 416/416 [01:06<00:00,  6.24it/s]\n",
      "Epoch 2/15:\n",
      "  Training Loss: 0.3992\n",
      "  Training Accuracy: 94.11%\n",
      "  Validation Loss: 0.1523\n",
      "  Validation Accuracy: 97.78%\n",
      "  *** Validation Loss improved from 0.4202 to 0.1523. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.399172, Acc: 0.9411, Prec: 0.9409, Rec: 0.9411, F1: 0.9404\n",
      "Eval  Loss: 0.152263, Acc: 0.9778, Prec: 0.9798, Rec: 0.9778, F1: 0.9776\n",
      "Grad Norm (last step): 0.694261, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.45 sec\n",
      "Epoch 3/15: 100%|█████████████████████████████| 416/416 [01:07<00:00,  6.18it/s]\n",
      "Epoch 3/15:\n",
      "  Training Loss: 0.1855\n",
      "  Training Accuracy: 97.84%\n",
      "  Validation Loss: 0.0719\n",
      "  Validation Accuracy: 98.73%\n",
      "  *** Validation Loss improved from 0.1523 to 0.0719. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.185494, Acc: 0.9784, Prec: 0.9783, Rec: 0.9784, F1: 0.9783\n",
      "Eval  Loss: 0.071899, Acc: 0.9873, Prec: 0.9884, Rec: 0.9873, F1: 0.9873\n",
      "Grad Norm (last step): 2.698818, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 77.19 sec\n",
      "Epoch 4/15: 100%|█████████████████████████████| 416/416 [01:06<00:00,  6.23it/s]\n",
      "Epoch 4/15:\n",
      "  Training Loss: 0.1052\n",
      "  Training Accuracy: 98.98%\n",
      "  Validation Loss: 0.0555\n",
      "  Validation Accuracy: 99.37%\n",
      "  *** Validation Loss improved from 0.0719 to 0.0555. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.105168, Acc: 0.9898, Prec: 0.9898, Rec: 0.9898, F1: 0.9898\n",
      "Eval  Loss: 0.055542, Acc: 0.9937, Prec: 0.9940, Rec: 0.9937, F1: 0.9936\n",
      "Grad Norm (last step): 0.438432, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.66 sec\n",
      "Epoch 5/15: 100%|█████████████████████████████| 416/416 [01:06<00:00,  6.23it/s]\n",
      "Epoch 5/15:\n",
      "  Training Loss: 0.0807\n",
      "  Training Accuracy: 98.98%\n",
      "  Validation Loss: 0.0691\n",
      "  Validation Accuracy: 97.78%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.080690, Acc: 0.9898, Prec: 0.9899, Rec: 0.9898, F1: 0.9898\n",
      "Eval  Loss: 0.069101, Acc: 0.9778, Prec: 0.9798, Rec: 0.9778, F1: 0.9777\n",
      "Grad Norm (last step): 7.358601, LR: 0.0001000000\n",
      "Epoch Time Taken: 76.46 sec\n",
      "Epoch 6/15: 100%|█████████████████████████████| 416/416 [01:06<00:00,  6.23it/s]\n",
      "Epoch 6/15:\n",
      "  Training Loss: 0.0654\n",
      "  Training Accuracy: 98.74%\n",
      "  Validation Loss: 0.0560\n",
      "  Validation Accuracy: 98.42%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.065434, Acc: 0.9874, Prec: 0.9875, Rec: 0.9874, F1: 0.9874\n",
      "Eval  Loss: 0.056043, Acc: 0.9842, Prec: 0.9854, Rec: 0.9841, F1: 0.9841\n",
      "Grad Norm (last step): 0.312526, LR: 0.0001000000\n",
      "Epoch Time Taken: 76.42 sec\n",
      "Epoch 7/15: 100%|█████████████████████████████| 416/416 [01:06<00:00,  6.23it/s]\n",
      "Epoch 7/15:\n",
      "  Training Loss: 0.0681\n",
      "  Training Accuracy: 98.62%\n",
      "  Validation Loss: 0.0474\n",
      "  Validation Accuracy: 98.10%\n",
      "  *** Validation Loss improved from 0.0555 to 0.0474. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.068127, Acc: 0.9862, Prec: 0.9862, Rec: 0.9862, F1: 0.9862\n",
      "Eval  Loss: 0.047439, Acc: 0.9810, Prec: 0.9840, Rec: 0.9810, F1: 0.9807\n",
      "Grad Norm (last step): 0.122164, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.59 sec\n",
      "Epoch 8/15: 100%|█████████████████████████████| 416/416 [01:07<00:00,  6.20it/s]\n",
      "Epoch 8/15:\n",
      "  Training Loss: 0.0343\n",
      "  Training Accuracy: 99.52%\n",
      "  Validation Loss: 0.0940\n",
      "  Validation Accuracy: 98.42%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.034267, Acc: 0.9952, Prec: 0.9952, Rec: 0.9952, F1: 0.9952\n",
      "Eval  Loss: 0.093986, Acc: 0.9842, Prec: 0.9881, Rec: 0.9841, F1: 0.9837\n",
      "Grad Norm (last step): 0.113798, LR: 0.0001000000\n",
      "Epoch Time Taken: 76.82 sec\n",
      "Epoch 9/15: 100%|█████████████████████████████| 416/416 [01:07<00:00,  6.17it/s]\n",
      "Epoch 9/15:\n",
      "  Training Loss: 0.0313\n",
      "  Training Accuracy: 99.52%\n",
      "  Validation Loss: 0.0587\n",
      "  Validation Accuracy: 97.78%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.031334, Acc: 0.9952, Prec: 0.9952, Rec: 0.9952, F1: 0.9952\n",
      "Eval  Loss: 0.058664, Acc: 0.9778, Prec: 0.9814, Rec: 0.9778, F1: 0.9775\n",
      "Grad Norm (last step): 0.039418, LR: 0.0001000000\n",
      "Epoch Time Taken: 77.13 sec\n",
      "Epoch 10/15: 100%|████████████████████████████| 416/416 [01:06<00:00,  6.23it/s]\n",
      "Epoch 10/15:\n",
      "  Training Loss: 0.0214\n",
      "  Training Accuracy: 99.70%\n",
      "  Validation Loss: 0.0295\n",
      "  Validation Accuracy: 99.05%\n",
      "  *** Validation Loss improved from 0.0474 to 0.0295. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.021447, Acc: 0.9970, Prec: 0.9970, Rec: 0.9970, F1: 0.9970\n",
      "Eval  Loss: 0.029475, Acc: 0.9905, Prec: 0.9914, Rec: 0.9905, F1: 0.9904\n",
      "Grad Norm (last step): 0.067195, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.68 sec\n",
      "Epoch 11/15: 100%|████████████████████████████| 416/416 [01:06<00:00,  6.21it/s]\n",
      "Epoch 11/15:\n",
      "  Training Loss: 0.0430\n",
      "  Training Accuracy: 99.10%\n",
      "  Validation Loss: 0.0217\n",
      "  Validation Accuracy: 99.37%\n",
      "  *** Validation Loss improved from 0.0295 to 0.0217. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.042959, Acc: 0.9910, Prec: 0.9911, Rec: 0.9910, F1: 0.9910\n",
      "Eval  Loss: 0.021700, Acc: 0.9937, Prec: 0.9940, Rec: 0.9937, F1: 0.9936\n",
      "Grad Norm (last step): 4.414335, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.79 sec\n",
      "Epoch 12/15: 100%|████████████████████████████| 416/416 [01:06<00:00,  6.22it/s]\n",
      "Epoch 12/15:\n",
      "  Training Loss: 0.0500\n",
      "  Training Accuracy: 98.80%\n",
      "  Validation Loss: 0.0278\n",
      "  Validation Accuracy: 99.05%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.050045, Acc: 0.9880, Prec: 0.9881, Rec: 0.9880, F1: 0.9880\n",
      "Eval  Loss: 0.027766, Acc: 0.9905, Prec: 0.9907, Rec: 0.9905, F1: 0.9905\n",
      "Grad Norm (last step): 0.151541, LR: 0.0001000000\n",
      "Epoch Time Taken: 76.56 sec\n",
      "Epoch 13/15: 100%|████████████████████████████| 416/416 [01:06<00:00,  6.22it/s]\n",
      "Epoch 13/15:\n",
      "  Training Loss: 0.0199\n",
      "  Training Accuracy: 99.64%\n",
      "  Validation Loss: 0.0343\n",
      "  Validation Accuracy: 99.05%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.019910, Acc: 0.9964, Prec: 0.9964, Rec: 0.9964, F1: 0.9964\n",
      "Eval  Loss: 0.034339, Acc: 0.9905, Prec: 0.9914, Rec: 0.9905, F1: 0.9906\n",
      "Grad Norm (last step): 0.113740, LR: 0.0001000000\n",
      "Epoch Time Taken: 76.49 sec\n",
      "Epoch 14/15: 100%|████████████████████████████| 416/416 [01:07<00:00,  6.17it/s]\n",
      "Epoch 14/15:\n",
      "  Training Loss: 0.0398\n",
      "  Training Accuracy: 98.92%\n",
      "  Validation Loss: 0.0302\n",
      "  Validation Accuracy: 99.37%\n",
      "  Validation Loss did not improve.\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.039845, Acc: 0.9892, Prec: 0.9892, Rec: 0.9892, F1: 0.9892\n",
      "Eval  Loss: 0.030188, Acc: 0.9937, Prec: 0.9940, Rec: 0.9937, F1: 0.9936\n",
      "Grad Norm (last step): 0.076534, LR: 0.0001000000\n",
      "Epoch Time Taken: 77.16 sec\n",
      "Epoch 15/15: 100%|████████████████████████████| 416/416 [01:06<00:00,  6.24it/s]\n",
      "Epoch 15/15:\n",
      "  Training Loss: 0.0224\n",
      "  Training Accuracy: 99.52%\n",
      "  Validation Loss: 0.0181\n",
      "  Validation Accuracy: 99.37%\n",
      "  *** Validation Loss improved from 0.0217 to 0.0181. Saving model. ***\n",
      "\n",
      "=== Logging final BEST model to MLflow ===\n",
      "Train Loss: 0.022362, Acc: 0.9952, Prec: 0.9952, Rec: 0.9952, F1: 0.9952\n",
      "Eval  Loss: 0.018086, Acc: 0.9937, Prec: 0.9940, Rec: 0.9937, F1: 0.9936\n",
      "Grad Norm (last step): 0.042064, LR: 0.0001000000\n",
      "Model saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_21Classes_Cam10718/Trial_21class_12_12_25.pt\n",
      "Epoch Time Taken: 76.57 sec\n",
      "Model ready for fine-tuning with 21 output classes.\n",
      "Activating Optimizer Adam\n",
      "\u001b[31m2025/12/12 16:01:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "BEST model logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/smartan5070/Downloads/SlowfastTrainer-main/train_MViT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53480944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the training log\n",
    "log_file = r'/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_30Classes_Cam10718/SlowFast_training_log.txt'\n",
    "\n",
    "# Initialize metric lists\n",
    "epochs = []\n",
    "train_loss, eval_loss = [], []\n",
    "train_acc, eval_acc = [], []\n",
    "train_prec, eval_prec = [], []\n",
    "train_rec, eval_rec = [], []\n",
    "train_f1, eval_f1 = [], []\n",
    "\n",
    "# Read and parse the file\n",
    "with open(log_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    epoch_match = re.match(r'Epoch (\\d+)/\\d+', line)\n",
    "    if epoch_match:\n",
    "        epochs.append(int(epoch_match.group(1)))\n",
    "\n",
    "    train_match = re.match(r'Train Loss: ([\\d.]+), Accuracy: ([\\d.]+), Precision: ([\\d.]+), Recall: ([\\d.]+), F1: ([\\d.]+)', line)\n",
    "    if train_match:\n",
    "        train_loss.append(float(train_match.group(1)))\n",
    "        train_acc.append(float(train_match.group(2)))\n",
    "        train_prec.append(float(train_match.group(3)))\n",
    "        train_rec.append(float(train_match.group(4)))\n",
    "        train_f1.append(float(train_match.group(5)))\n",
    "\n",
    "    eval_match = re.match(r'Eval Loss: ([\\d.]+), Accuracy: ([\\d.]+), Precision: ([\\d.]+), Recall: ([\\d.]+), F1: ([\\d.]+)', line)\n",
    "    if eval_match:\n",
    "        eval_loss.append(float(eval_match.group(1)))\n",
    "        eval_acc.append(float(eval_match.group(2)))\n",
    "        eval_prec.append(float(eval_match.group(3)))\n",
    "        eval_rec.append(float(eval_match.group(4)))\n",
    "        eval_f1.append(float(eval_match.group(5)))\n",
    "\n",
    "# Plotting\n",
    "def plot_metric(train, eval, title, ylabel):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train, label='Train')\n",
    "    plt.plot(epochs, eval, label='Eval')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha = 0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"30_class_train_loss_val_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(train_loss, eval_loss, 'Loss over Epochs', 'Loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94006c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "def plot_metric(train, eval, title, ylabel):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train, label='Train')\n",
    "    plt.plot(epochs, eval, label='Eval')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha = 0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"30_class_train_acc_val_acc.png\")\n",
    "    # plt.savefig(\"30_class_train_prec_val_prec.png\")\n",
    "    # plt.savefig(\"30_class_train_recall_val_recall.png\")\n",
    "    # plt.savefig(\"30_class_train_F1_val_F1.png\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_metric(train_acc, eval_acc, 'Accuracy over Epochs', 'Accuracy')\n",
    "# plot_metric(train_prec, eval_prec, 'Precision over Epochs', 'Precision')\n",
    "# plot_metric(train_rec, eval_rec, 'Recall over Epochs', 'Recall')\n",
    "# plot_metric(train_f1, eval_f1, 'F1 Score over Epochs', 'F1 Score')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c07595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoReader, cpu\n",
    "from config import *\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "\n",
    "\n",
    "# Check your exact num_classes.\n",
    "num_classes = 30 \n",
    "frames_per_clip = 16\n",
    "\n",
    "# Specifying the model path\n",
    "model_path = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_30Classes_Cam10718/Testing_30Classes_Cam10718.pt\"\n",
    "\n",
    "# Defining the test datapath\n",
    "test_datapath = \"Dataset_30Classes_Cam107-18_SPLIT/test\"\n",
    "\n",
    "\n",
    "class YourVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        print(\"########### BUILD INDEX TRACKING ###########\")\n",
    "        classes = sorted([d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))])\n",
    "        print(f\" |Classes: {classes}\")\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        print(f\" |Class_to_idx: {self.class_to_idx}\")\n",
    "        for cls_name in classes:\n",
    "            cls_dir = os.path.join(self.root_dir, cls_name)\n",
    "            print(f\" |Class_directory: {cls_dir}\")\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith(\".mp4\"):\n",
    "                    self.video_paths.append(os.path.join(cls_dir, fname))\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "        print(f\" |Num videos: {len(self.video_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            # Frame indices\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                # pad by repeating last frame (simple + robust)\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()          # (T,H,W,C)\n",
    "\n",
    "            # Fix for grayscale videos\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   # (C,T,H,W)\n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)                                    # keep (C,T,H,W)\n",
    "\n",
    "            return frames, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            # try next video (avoid infinite recursion if dataset has 0 length)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # # 5. Load the state dictionary\n",
    "    # # Use map_location to ensure it loads correctly regardless of the saved device\n",
    "    # model_state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def pack_pathway_output(frames, alpha=4):\n",
    "    \"\"\"\n",
    "    Create inputs for SlowFast model from a clip.\n",
    "\n",
    "    Args:\n",
    "        frames (Tensor): shape (T, C, H, W)\n",
    "        alpha (int): temporal stride between fast and slow pathways (usually 4)\n",
    "\n",
    "    Returns:\n",
    "        List[Tensor]: [slow_pathway, fast_pathway]\n",
    "    \"\"\"\n",
    "    fast_pathway = frames # full frame sequence\n",
    "    # print(f\"29. Fast Pathway Shape: {fast_pathway.shape}\")\n",
    "    slow_indices = torch.linspace(0, fast_pathway.shape[2] - 1, fast_pathway.shape[2] // 4).long().to(device)\n",
    "    # print(f\"30. Line Spaced Slow Indices Using ({0, fast_pathway.shape[2] - 1, fast_pathway.shape[2] // 4})\")\n",
    "    slow_pathway = torch.index_select(fast_pathway, 2, slow_indices)  # T dimension is dim=2\n",
    "    # print(f\"31. Slow Pathway Shape: {slow_pathway.shape}\")\n",
    "\n",
    "    # print(f\"Returning Pathways: [{slow_pathway.shape, fast_pathway.shape}]\")\n",
    "    return [slow_pathway, fast_pathway]\n",
    "\n",
    "def run_infernce(test_loader):\n",
    "    # Lists to store all true labels and predictions\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Inference'):\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # inputs_permuted = inputs.permute(0, 2, 1, 3, 4).to(device) \n",
    "            # inputs = pack_pathway_output(inputs)\n",
    "\n",
    "            # Perform the forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Store the ground truth labels and predictions for metric calculation later\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Main\n",
    "# if __name__ == \"__main__\":\\\n",
    "    \n",
    "model = load_model(model_path, num_classes, K)\n",
    "\n",
    "# Applying transform\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    CenterCrop(224),\n",
    "    NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = YourVideoDataset(test_datapath, transform=transform, frames_per_clip=frames_per_clip)\n",
    "\n",
    "test_loader   = DataLoader(test_dataset,   batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "all_labels, all_predictions = run_infernce(test_loader)\n",
    "\n",
    "# Calculate Overall Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"\\nOverall Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Generate a detailed Classification Report (Precision, Recall, F1-Score)\n",
    "# Map index back to class names for readability if you have a class_to_idx map\n",
    "target_names = list(test_dataset.class_to_idx.keys())\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "# Generate and print a Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "print(conf_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d819eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "_transforms_video is available\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Fallback _transforms_video not available\n",
      "Using MLflow Run ID: 923a28bac6774f02a24a8e242bd76338\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "Loading model from MLflow: runs:/923a28bac6774f02a24a8e242bd76338/best_model\n",
      "Downloading artifacts:   0%|                              | 0/1 [00:00<?, ?it/s]\n",
      "Downloading artifacts: 100%|█████████████████████| 6/6 [00:00<00:00, 180.10it/s]\n",
      "########### BUILD INDEX TRACKING (TRAINING ORDER) ###########\n",
      " |Test classes: ['arm_circles', 'bicep_curls', 'db_chest_press', 'db_incline_chest_press', 'db_lunges', 'front_raise', 'hammer_curls', 'kb_goblet_squats', 'kb_goodmorning', 'kb_overhead_press', 'kb_swings', 'lateral_raise']\n",
      " |Using training mapping: {'arm_circles': 0, 'bb_military_press': 1, 'bb_upright': 2, 'bicep_curls': 3, 'db_chest_press': 4, 'db_incline_chest_press': 5, 'db_lunges': 6, 'db_reverse_flys': 7, 'db_seated_shoulder_press': 8, 'ez_bb_curls': 9, 'front_raise': 10, 'hammer_curls': 11, 'kb_goblet_squats': 12, 'kb_goodmorning': 13, 'kb_overhead_press': 14, 'kb_swings': 15, 'lateral_raise': 16, 'skull_crushers': 17, 'spider_curls': 18, 'tricep_dips': 19, 'zottman_curls': 20}\n",
      "Inference: 100%|██████████████████████████████████| 5/5 [00:00<00:00,  5.02it/s]\n",
      "\n",
      "Overall Test Accuracy: 84.21%\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             arm_circles       1.00      1.00      1.00         1\n",
      "             bicep_curls       1.00      1.00      1.00         1\n",
      "          db_chest_press       0.00      0.00      0.00         2\n",
      "  db_incline_chest_press       0.33      0.50      0.40         2\n",
      "               db_lunges       1.00      1.00      1.00         2\n",
      "db_seated_shoulder_press       0.00      0.00      0.00         0\n",
      "             front_raise       1.00      1.00      1.00         1\n",
      "            hammer_curls       1.00      1.00      1.00         1\n",
      "        kb_goblet_squats       1.00      1.00      1.00         2\n",
      "          kb_goodmorning       1.00      1.00      1.00         2\n",
      "       kb_overhead_press       1.00      1.00      1.00         2\n",
      "               kb_swings       1.00      1.00      1.00         2\n",
      "           lateral_raise       1.00      1.00      1.00         1\n",
      "\n",
      "                accuracy                           0.84        19\n",
      "               macro avg       0.79      0.81      0.80        19\n",
      "            weighted avg       0.82      0.84      0.83        19\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "!python test_MViT.py --run_id '923a28bac6774f02a24a8e242bd76338'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8644c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from decord import VideoReader, cpu\n",
    "from torch.utils.data import Dataset\n",
    "# Ensure config, tqdm, and other imports are available\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize\n",
    "\n",
    "\n",
    "class FlatVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads videos directly from a single flat folder, assigning a dummy label (-1).\n",
    "    Used for inference where true labels are unknown or unnecessary.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        print(\"########### BUILD INDEX TRACKING (FLAT DIRECTORY) ###########\")\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if fname.lower().endswith(\".mp4\"):\n",
    "                self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "        print(f\" |Num videos found: {len(self.video_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        # Assign a dummy label, as the ground truth is unknown for inference\n",
    "        label = -1 \n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            # --- Frame Indexing Logic (Keep the same) ---\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()          # (T,H,W,C)\n",
    "\n",
    "            # Fix for grayscale videos\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            # (C,T,H,W), normalize to [0, 1]\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            # Return frames, dummy label, and the original file path for reference\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            # Try next video (avoid infinite recursion if dataset has 0 length)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def run_infernce_flat(test_loader, model):\n",
    "    # Lists to store predicted class labels and the video file paths\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "\n",
    "    # Get the device from the model\n",
    "    device = next(model.parameters()).device \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # The DataLoader now yields 3 items: inputs, dummy_labels, paths\n",
    "        for inputs, _, paths in tqdm(test_loader, desc='Inference'):\n",
    "            \n",
    "            inputs = inputs.to(device) \n",
    "            \n",
    "            # Perform the forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class index\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Store the predictions and paths\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    # Return a dictionary mapping file path to predicted class index\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "# --- IMPORTANT: Ensure K and NormalizeVideo are defined/imported ---\n",
    "# K must match your training (e.g., K=3)\n",
    "K = 3 \n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3008d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### BUILD INDEX TRACKING (FLAT DIRECTORY) ###########\n",
      " |Num videos found: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 2/2 [00:00<00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "################# INFERENCE RESULTS #################\n",
      "\n",
      "Loaded class index mapping:\n",
      "0: BicepsCurls\n",
      "1: FrontRaises\n",
      "2: HammerCurls\n",
      "3: LateralRaise\n",
      "4: UprightRows\n",
      "5: arm_circles\n",
      "6: bb_military_press\n",
      "7: bb_upright\n",
      "8: bus_driver\n",
      "9: chest_rows\n",
      "10: concentration_curls\n",
      "11: db_rows\n",
      "12: db_seated_arnoldpress\n",
      "13: db_seated_hammercurl\n",
      "14: dumbbell_chest_press\n",
      "15: dumbbell_incline_chest_press\n",
      "16: dumbbell_lunges\n",
      "17: dumbbell_reverse_flys\n",
      "18: ez_bb_curls\n",
      "19: kb_gobletsquats\n",
      "20: kb_ohpress\n",
      "21: kb_swings\n",
      "22: kettlebell_goodmorning\n",
      "23: seated_dumbbell_shoulderpess\n",
      "24: singlearm_dumbbell_rows\n",
      "25: skull_crushers\n",
      "26: spider_curls\n",
      "27: standing_db_shoulder_press\n",
      "28: tricep_dips\n",
      "29: zottman_curls\n",
      "\n",
      "File: antotest_114_clip01_00026_00370_seg004.mp4         -> Predicted Class: bb_upright\n",
      "File: antotest_114_clip01_00026_00370_seg005.mp4         -> Predicted Class: bb_upright\n",
      "File: antotest_114_clip01_00026_00370_seg003.mp4         -> Predicted Class: bb_upright\n",
      "File: antotest_114_clip01_00026_00370_seg006.mp4         -> Predicted Class: bb_upright\n",
      "File: antotest_114_clip01_00026_00370_seg001.mp4         -> Predicted Class: bb_upright\n",
      "File: antotest_114_clip01_00026_00370_seg002.mp4         -> Predicted Class: db_rows\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing the videos you want to test\n",
    "flat_test_folder = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/anto_yolo\" # <-- CHANGE THIS PATH!\n",
    "\n",
    "# 1. Load Model (Using the corrected function signature)\n",
    "model = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_30Classes_Cam10718/Testing_30Classes_Cam10718.pt', \n",
    "                   num_classes=30, K=K)\n",
    "\n",
    "# Set the device variable for use in DataLoader\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 2. Define Transforms (Keep the same)\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    CenterCrop(224),\n",
    "    NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Load Flat Dataset\n",
    "frames_per_clip = 16 \n",
    "test_dataset = FlatVideoDataset(flat_test_folder, transform=transform, frames_per_clip=frames_per_clip)\n",
    "\n",
    "# 4. Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, \n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "# 5. Run Inference\n",
    "video_predictions = run_infernce_flat(test_loader, model)\n",
    "\n",
    "# 6. Output Results\n",
    "print(\"\\n\\n################# INFERENCE RESULTS #################\")\n",
    "\n",
    "# -------------------- NEW: AUTO-LOAD TRAINING CLASS NAMES --------------------\n",
    "import os\n",
    "\n",
    "train_root = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Dataset_30Classes_Cam107-18_SPLIT/train\"\n",
    "\n",
    "# Auto-read folder names in alphabetical order — EXACT training class order\n",
    "class_names = sorted(os.listdir(train_root))\n",
    "\n",
    "# Map index → class name automatically\n",
    "idx_to_class_name = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "# print(\"\\nLoaded class index mapping:\")\n",
    "# for i, name in idx_to_class_name.items():\n",
    "#     print(f\"{i}: {name}\")\n",
    "# print()\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 7. Print predictions with correct class names\n",
    "for path, prediction_idx in video_predictions.items():\n",
    "    \n",
    "    # Look up the corresponding class name\n",
    "    predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "    \n",
    "    # Print the result using the class name\n",
    "    print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "print(\"#####################################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Test Accuracy: 98.10%\n",
      "\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                 BicepsCurls       0.90      0.90      0.90        10\n",
      "                 FrontRaises       0.91      1.00      0.95        10\n",
      "                 HammerCurls       1.00      1.00      1.00        10\n",
      "                LateralRaise       1.00      1.00      1.00        10\n",
      "                 UprightRows       1.00      1.00      1.00        10\n",
      "                 arm_circles       1.00      1.00      1.00        10\n",
      "           bb_military_press       1.00      1.00      1.00        10\n",
      "        dumbbell_chest_press       1.00      1.00      1.00        10\n",
      "dumbbell_incline_chest_press       1.00      1.00      1.00        10\n",
      "             dumbbell_lunges       0.90      0.90      0.90        10\n",
      "       dumbbell_reverse_flys       1.00      0.90      0.95        10\n",
      "                 ez_bb_curls       1.00      1.00      1.00        10\n",
      "             kb_gobletsquats       1.00      1.00      1.00        10\n",
      "                  kb_ohpress       1.00      0.90      0.95        10\n",
      "                   kb_swings       1.00      1.00      1.00        10\n",
      "      kettlebell_goodmorning       1.00      1.00      1.00        10\n",
      "seated_dumbbell_shoulderpess       0.91      1.00      0.95        10\n",
      "     singlearm_dumbbell_rows       1.00      1.00      1.00        10\n",
      "              skull_crushers       1.00      1.00      1.00        10\n",
      "                spider_curls       1.00      1.00      1.00        10\n",
      "                 tricep_dips       1.00      1.00      1.00        10\n",
      "\n",
      "                    accuracy                           0.98       210\n",
      "                   macro avg       0.98      0.98      0.98       210\n",
      "                weighted avg       0.98      0.98      0.98       210\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  9  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "# # Calculate Overall Accuracy\n",
    "# accuracy = accuracy_score(all_labels, all_predictions)\n",
    "# print(f\"\\nOverall Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# # Generate a detailed Classification Report (Precision, Recall, F1-Score)\n",
    "# # Map index back to class names for readability if you have a class_to_idx map\n",
    "# target_names = list(test_dataset.class_to_idx.keys())\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "# # Generate and print a Confusion Matrix\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "# print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4accb2b",
   "metadata": {},
   "source": [
    "# CROP VIDEOS ACROSS PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b306014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 videos to process.\n",
      "--------------------------------------------------\n",
      "Processing bench_press.mp4 with 1117 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping bench_press.mp4: 100%|██████████| 1117/1117 [00:00<00:00, 2976.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/bench_press.mp4\n",
      "--------------------------------------------------\n",
      "Processing bicep_curls.mp4 with 757 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping bicep_curls.mp4: 100%|██████████| 757/757 [00:00<00:00, 2765.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/bicep_curls.mp4\n",
      "--------------------------------------------------\n",
      "Processing db_rows.mp4 with 720 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping db_rows.mp4: 100%|██████████| 720/720 [00:00<00:00, 1908.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/db_rows.mp4\n",
      "--------------------------------------------------\n",
      "Processing frontraise.mp4 with 749 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping frontraise.mp4: 100%|██████████| 749/749 [00:00<00:00, 1338.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/frontraise.mp4\n",
      "--------------------------------------------------\n",
      "Processing goblet.mp4 with 974 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping goblet.mp4: 100%|██████████| 974/974 [00:00<00:00, 3401.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/goblet.mp4\n",
      "--------------------------------------------------\n",
      "Processing goodmorning.mp4 with 3907 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping goodmorning.mp4: 100%|██████████| 3907/3907 [00:01<00:00, 2940.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/goodmorning.mp4\n",
      "--------------------------------------------------\n",
      "Processing hammer_curls.mp4 with 900 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping hammer_curls.mp4: 100%|██████████| 900/900 [00:00<00:00, 2600.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/hammer_curls.mp4\n",
      "--------------------------------------------------\n",
      "Processing inclined_bench_press.mp4 with 1106 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping inclined_bench_press.mp4: 100%|██████████| 1106/1106 [00:00<00:00, 2980.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/inclined_bench_press.mp4\n",
      "--------------------------------------------------\n",
      "Processing kbswing.mp4 with 768 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping kbswing.mp4: 100%|██████████| 768/768 [00:00<00:00, 1569.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/kbswing.mp4\n",
      "--------------------------------------------------\n",
      "Processing lateral_raise.mp4 with 650 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping lateral_raise.mp4: 100%|██████████| 650/650 [00:00<00:00, 1265.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/lateral_raise.mp4\n",
      "--------------------------------------------------\n",
      "Processing lunges.mp4 with 933 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping lunges.mp4: 100%|██████████| 933/933 [00:00<00:00, 3004.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/lunges.mp4\n",
      "--------------------------------------------------\n",
      "Processing ohp_press.mp4 with 1228 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping ohp_press.mp4: 100%|██████████| 1228/1228 [00:00<00:00, 2307.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos/ohp_press.mp4\n",
      "\n",
      "Batch processing complete for all videos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import glob # Import the glob module\n",
    "from ultralytics import YOLO \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "YOLO_MODEL = 'yolov8n.pt'  \n",
    "CONFIDENCE_THRESHOLD = 0.5 \n",
    "PADDING = 30               \n",
    "PERSON_CLASS_ID = 0        \n",
    "# Define accepted video extensions\n",
    "VIDEO_EXTENSIONS = ('*.mp4', '*.avi', '*.mov', '*.mkv')\n",
    "\n",
    "def detect_and_crop_person(input_video_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, and crops all frames of the video to that consistent area.\n",
    "    \"\"\"\n",
    "    # 1. Initialize YOLOv8 Model (loaded inside function for clean scope per video process)\n",
    "    try:\n",
    "        # Suppress ultralytics output during batch processing for cleaner console\n",
    "        model = YOLO(YOLO_MODEL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model. Ensure 'ultralytics' is installed: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Initialize Video Reader\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    out = None\n",
    "    fixed_crop_area = None # Stores the fixed [x1, y1, x2, y2] area (tuple of ints)\n",
    "    output_dims = None     # Stores the final (width, height) tuple for the VideoWriter\n",
    "\n",
    "    print(f\"Processing {os.path.basename(input_video_path)} with {total_frames} frames...\")\n",
    "\n",
    "    # 3. Process Frames\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Cropping {os.path.basename(input_video_path)}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        current_crop_area = None\n",
    "\n",
    "        if fixed_crop_area is None:\n",
    "            # Try to find a person to establish the fixed crop area\n",
    "            results = model(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == PERSON_CLASS_ID:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "            \n",
    "            if best_box is not None:\n",
    "                # Set the fixed crop area based on the first detection with padding and clamping\n",
    "                x1, y1, x2, y2 = best_box\n",
    "                fixed_crop_area = (\n",
    "                    max(0, x1 - PADDING),\n",
    "                    max(0, y1 - PADDING),\n",
    "                    min(frame_width, x2 + PADDING),\n",
    "                    min(frame_height, y2 + PADDING)\n",
    "                )\n",
    "                current_crop_area = fixed_crop_area\n",
    "                # Calculate the output dimensions once (W, H)\n",
    "                crop_width = fixed_crop_area[2] - fixed_crop_area[0]\n",
    "                crop_height = fixed_crop_area[3] - fixed_crop_area[1]\n",
    "                output_dims = (crop_width, crop_height)\n",
    "        else:\n",
    "            # Use the already established fixed crop area for all subsequent frames\n",
    "            current_crop_area = fixed_crop_area\n",
    "\n",
    "        # --- Frame Cropping and Writing Logic ---\n",
    "        frame_to_write = None\n",
    "\n",
    "        if current_crop_area is not None:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = current_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "            frame_to_write = cropped_frame\n",
    "\n",
    "            # Initialize VideoWriter if not already done\n",
    "            if out is None and output_dims is not None:\n",
    "                # Ensure width/height are positive for the writer to work\n",
    "                if output_dims[0] > 0 and output_dims[1] > 0:\n",
    "                    out = cv2.VideoWriter(output_video_path, fourcc, fps, output_dims)\n",
    "                else:\n",
    "                    print(f\"\\nWarning: Invalid output dimensions {output_dims} for {os.path.basename(input_video_path)}. Skipping video.\")\n",
    "                    break # Exit the frame loop for this video\n",
    "\n",
    "        if out is not None:\n",
    "            if frame_to_write is None or frame_to_write.size == 0:\n",
    "                # If no valid crop occurred, write a black frame of the defined size\n",
    "                # Use output_dims[1] for H, output_dims[0] for W in shape tuple\n",
    "                frame_to_write = np.zeros((output_dims[1], output_dims[0], 3), dtype=np.uint8)\n",
    "            \n",
    "            # This resize is a fallback but shouldn't be strictly necessary with fixed area logic\n",
    "            if frame_to_write.shape[0] != output_dims[1] or frame_to_write.shape[1] != output_dims[0]:\n",
    "                frame_to_write = cv2.resize(frame_to_write, output_dims)\n",
    "            \n",
    "            out.write(frame_to_write)\n",
    "        \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "        print(f\"\\nFinished processing. Cropped video saved to: {output_video_path}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Could not finalize video processing for {os.path.basename(input_video_path)} (perhaps no person was detected in the whole video). No output file created.\")\n",
    "\n",
    "\n",
    "# --- Batch Processing Logic ---\n",
    "if __name__ == '__main__':\n",
    "    # Define your input and output directories\n",
    "    INPUT_DIR = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/correct_videos\"\n",
    "    OUTPUT_DIR = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test_set_data/correct_videos 1/cropped_correct_videos\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True) #\n",
    "\n",
    "    # Find all video files in the input directory\n",
    "    video_files = []\n",
    "    for ext in VIDEO_EXTENSIONS:\n",
    "        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext))) #\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found with extensions {VIDEO_EXTENSIONS} in {INPUT_DIR}\")\n",
    "    else:\n",
    "        print(f\"Found {len(video_files)} videos to process.\")\n",
    "\n",
    "    # Process each video file found\n",
    "    for input_path in sorted(video_files): # Sorting ensures consistent order\n",
    "        filename = os.path.basename(input_path)\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        detect_and_crop_person(input_path, output_path)\n",
    "\n",
    "    print(\"\\nBatch processing complete for all videos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f140a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video file /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/arm_circles_114_clip23_05660_05935.mp4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MViT_V1_B_Weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 282\u001b[0m\n\u001b[1;32m    279\u001b[0m detect_and_crop_person(input_video_path, output_video_path)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# 2. Load Model (Using the corrected function signature)\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# 3. Define Transforms\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, num_classes, K)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(model_path, num_classes, K):\n\u001b[0;32m--> 212\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mMViT_V1_B_Weights\u001b[49m\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Load the model directly\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     model \u001b[38;5;241m=\u001b[39m mvit_v1_b(weights\u001b[38;5;241m=\u001b[39mweights)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MViT_V1_B_Weights' is not defined"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# YOLO crop single video\n",
    "######################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "\n",
    "def clamp_bbox(bbox, h, w):\n",
    "    \"\"\"\n",
    "    Clamp bounding box coordinates to ensure they are within image boundaries.\n",
    "\n",
    "    Args:\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2)\n",
    "        h (int): Image height\n",
    "        w (int): Image width\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: Clamped bounding box coordinates (x1, y1, x2, y2) or None if invalid\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    x1 = max(0, min(w - 1, int(x1)))\n",
    "    y1 = max(0, min(h - 1, int(y1)))\n",
    "    x2 = max(0, min(w,     int(x2)))\n",
    "    y2 = max(0, min(h,     int(y2)))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "# ============================\n",
    "# Function to Process and Crop Video\n",
    "# ============================\n",
    "\n",
    "def detect_and_crop_person(input_video_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, and crops all frames of the video to that consistent area.\n",
    "    \"\"\"\n",
    "    # 1. Initialize YOLOv8 Model (loaded inside function for clean scope per video process)\n",
    "    try:\n",
    "        model = YOLO('yolov8n.pt')  # Ensure YOLOv8 model is loaded\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Open video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output file\n",
    "    out = None  # Output video writer\n",
    "    fixed_crop_area = None  # Variable to store the fixed crop area\n",
    "    output_dims = None  # Output video dimensions (width, height)\n",
    "\n",
    "    print(f\"Processing video: {input_video_path} ({total_frames} frames)\")\n",
    "\n",
    "    # 3. Process and crop frames\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Processing {os.path.basename(input_video_path)}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        current_crop_area = None\n",
    "\n",
    "        # Detect person and set crop area in the first frame\n",
    "        if fixed_crop_area is None:\n",
    "            results = model(frame, conf=0.5, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            # Find largest detected person box\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == 0:  # PERSON_CLASS_ID = 0\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "            \n",
    "            if best_box is not None:\n",
    "\n",
    "                # 1️⃣ Clamp raw YOLO box\n",
    "                clamped_box = clamp_bbox(best_box, frame_height, frame_width)\n",
    "\n",
    "                if clamped_box is not None:\n",
    "                    x1, y1, x2, y2 = clamped_box\n",
    "\n",
    "                    PADDING = 10\n",
    "\n",
    "                    # 2️⃣ Add padding\n",
    "                    padded_box = (\n",
    "                        x1 - PADDING,\n",
    "                        y1 - PADDING,\n",
    "                        x2 + PADDING,\n",
    "                        y2 + PADDING\n",
    "                    )\n",
    "\n",
    "                    # 3️⃣ Clamp padded box again\n",
    "                    fixed_crop_area = clamp_bbox(padded_box, frame_height, frame_width)\n",
    "\n",
    "                    if fixed_crop_area is not None:\n",
    "                        current_crop_area = fixed_crop_area\n",
    "\n",
    "                        crop_width = fixed_crop_area[2] - fixed_crop_area[0]\n",
    "                        crop_height = fixed_crop_area[3] - fixed_crop_area[1]\n",
    "                        output_dims = (crop_width, crop_height)\n",
    "        else:\n",
    "            # Use the fixed crop area for all subsequent frames\n",
    "            current_crop_area = fixed_crop_area\n",
    "\n",
    "        if current_crop_area is not None:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = current_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "\n",
    "            # Initialize VideoWriter if not already done\n",
    "            if out is None:\n",
    "                out = cv2.VideoWriter(output_video_path, fourcc, fps, output_dims)\n",
    "\n",
    "            # Ensure the cropped frame is valid, else fill with black\n",
    "            if cropped_frame.size == 0:\n",
    "                cropped_frame = np.zeros((output_dims[1], output_dims[0], 3), dtype=np.uint8)\n",
    "\n",
    "            # Resize the cropped frame to match output dimensions if necessary\n",
    "            if cropped_frame.shape[0] != output_dims[1] or cropped_frame.shape[1] != output_dims[0]:\n",
    "                cropped_frame = cv2.resize(cropped_frame, output_dims)\n",
    "\n",
    "            out.write(cropped_frame)\n",
    "        \n",
    "    cap.release()\n",
    "\n",
    "    if out:\n",
    "        out.release()\n",
    "        print(f\"\\nCropped video saved to: {output_video_path}\")\n",
    "    else:\n",
    "        print(f\"\\nError: Could not crop the video {input_video_path}. No output created.\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Dataset and Inference\n",
    "# ============================\n",
    "\n",
    "class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if fname.lower().endswith(\".mp4\"):\n",
    "                self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = -1  # Dummy label\n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # # 5. Load the state dictionary\n",
    "    # # Use map_location to ensure it loads correctly regardless of the saved device\n",
    "    # model_state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "def run_inference(test_loader, model, device):\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "# ============================\n",
    "# Main Execution Logic\n",
    "# ============================\n",
    "\n",
    "# Define input and output paths\n",
    "input_video_path = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/arm_circles_114_clip23_05660_05935.mp4\"\n",
    "output_video_path = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/arm_circles_114_clip23_05660_05935.mp4\"\n",
    "flat_test_folder = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped\"  # Folder where cropped videos are saved\n",
    "\n",
    "# 1. Detect and Crop Person in Video\n",
    "detect_and_crop_person(input_video_path, output_video_path)\n",
    "\n",
    "# 2. Load Model (Using the corrected function signature)\n",
    "model = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=3)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 3. Define Transforms\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    CenterCrop(224),\n",
    "    NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# 4. Create Dataset and DataLoader\n",
    "frames_per_clip = 16\n",
    "test_dataset = FlatVideoDataset(flat_test_folder, transform=transform, frames_per_clip=frames_per_clip)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "# 5. Run Inference\n",
    "video_predictions = run_inference(test_loader, model, device)\n",
    "\n",
    "# 6. Output Results\n",
    "idx_to_class_name = {\n",
    "    0: \"BicepsCurls\", 1: \"FrontRaises\", 2: \"HammerCurls\", 3: \"LateralRaise\", \n",
    "    4: \"UprightRows\", 5: \"arm_circles\", 6: \"bb_military_press\", 7: \"dumbbell_chest_press\", \n",
    "    8: \"dumbbell_incline_chest_press\", 9: \"dumbbell_lunges\", 10: \"dumbbell_reverse_flys\", \n",
    "    11: \"ez_bb_curls\", 12: \"kb_gobletsquats\", 13: \"kb_ohpress\", 14: \"kb_swings\", \n",
    "    15: \"kettlebell_goodmorning\", 16: \"seated_dumbbell_shoulderpess\", 17: \"singlearm_dumbbell_rows\", \n",
    "    18: \"skull_crushers\", 19: \"spider_curls\", 20: \"tricep_dips\"\n",
    "}\n",
    "\n",
    "for path, prediction_idx in video_predictions.items():\n",
    "    predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "    print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc166cd",
   "metadata": {},
   "source": [
    "# New RTSP try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RTSP stream from rtsp://127.0.0.1:8554/test\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_1.mp4                                -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_2.mp4                                -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.17it/s]\n",
      "[h264 @ 0x6536efe8b480] negative number of zero coeffs at 43 43\n",
      "[h264 @ 0x6536efe8b480] error while decoding MB 43 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_3.mp4                                -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video_4.mp4                                -> Predicted Class: ez_bb_curls\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x6536e38029c0] Invalid level prefix\n",
      "[h264 @ 0x6536e38029c0] error while decoding MB 17 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.33it/s]\n",
      "[h264 @ 0x6536e8399c80] corrupted macroblock 31 55 (total_coeff=-1)\n",
      "[h264 @ 0x6536e8399c80] error while decoding MB 31 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_5.mp4                                -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.18it/s]\n",
      "[h264 @ 0x6536ea62f100] Invalid level prefix\n",
      "[h264 @ 0x6536ea62f100] error while decoding MB 59 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([4], device='cuda:0')\n",
      "File: cropped_video_6.mp4                                -> Predicted Class: UprightRows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_7.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.14it/s]\n",
      "[h264 @ 0x6536e83f2e00] corrupted macroblock 116 54 (total_coeff=-1)\n",
      "[h264 @ 0x6536e83f2e00] error while decoding MB 116 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video_7.mp4                                -> Predicted Class: ez_bb_curls\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video_8.mp4                                -> Predicted Class: ez_bb_curls\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x6536e84c2580] mb_type 105 in I slice too large at 19 40\n",
      "[h264 @ 0x6536e84c2580] error while decoding MB 19 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_9.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.40it/s]\n",
      "[h264 @ 0x6536e38029c0] Invalid level prefix\n",
      "[h264 @ 0x6536e38029c0] error while decoding MB 46 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_9.mp4                                -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_10.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.15it/s]\n",
      "[h264 @ 0x6536e38029c0] Invalid level prefix\n",
      "[h264 @ 0x6536e38029c0] error while decoding MB 43 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_10.mp4                               -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_11.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.86it/s]\n",
      "[h264 @ 0x653703e8cc40] Invalid level prefix\n",
      "[h264 @ 0x653703e8cc40] error while decoding MB 58 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video_11.mp4                               -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video_12.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 291\u001b[0m\n\u001b[1;32m    288\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Folder to save cropped frames if needed\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# 1. Detect and Crop Person from RTSP Stream\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[43mdetect_and_crop_person_from_rtsp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrtsp_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mdetect_and_crop_person_from_rtsp\u001b[0;34m(rtsp_url, output_folder, confidence_threshold, duration_seconds, model_path)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cropped 2-second video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Run inference on the saved 2-second cropped video with MViT\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mrun_mvit_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "Cell \u001b[0;32mIn[3], line 132\u001b[0m, in \u001b[0;36mrun_mvit_inference\u001b[0;34m(video_path, model_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mLoads the MViT model and performs inference on the given video.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Load the MViT model\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust `num_classes` and `K` based on your setup\u001b[39;00m\n\u001b[1;32m    133\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model_2\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Define video transformations\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 247\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, num_classes, K)\u001b[0m\n\u001b[1;32m    244\u001b[0m         p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Load the state dictionary\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Load the state dictionary INTO the instantiated model object\u001b[39;00m\n\u001b[1;32m    249\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state_dict)\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:1521\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:2122\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2121\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2122\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2123\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:535\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     ):\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    533\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         )\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    537\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:2086\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2085\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2086\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:2052\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2054\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/serialization.py:637\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/storage.py:291\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    290\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/_utils.py:102\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse, (\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 102\u001b[0m \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "import time\n",
    "\n",
    "def clamp_bbox(bbox, h, w):\n",
    "    \"\"\"\n",
    "    Clamp bounding box coordinates to ensure they are within image boundaries.\n",
    "\n",
    "    Args:\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2)\n",
    "        h (int): Image height\n",
    "        w (int): Image width\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: Clamped bounding box coordinates (x1, y1, x2, y2) or None if invalid\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    x1 = max(0, min(w - 1, int(x1)))\n",
    "    y1 = max(0, min(h - 1, int(y1)))\n",
    "    x2 = max(0, min(w,     int(x2)))\n",
    "    y2 = max(0, min(h,     int(y2)))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def detect_and_crop_person_from_rtsp(rtsp_url, output_folder, confidence_threshold=0.5, duration_seconds=2, model_path=None):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, crops all frames of the RTSP stream to that consistent area,\n",
    "    saves a 2-second cropped video to a specified output folder, and then performs inference using the MViT model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_yolo = YOLO('yolov8n.pt')  # Load YOLOv8 model for person detection\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Open RTSP stream\n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open RTSP stream {rtsp_url}\")\n",
    "        return\n",
    "\n",
    "    # Frame rate (fps) of the video stream\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        print(\"Error: Could not retrieve FPS from the RTSP stream.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the number of frames for 2 seconds\n",
    "    num_frames = int(fps * duration_seconds)\n",
    "\n",
    "    frame_count = 0\n",
    "    fixed_crop_area = None\n",
    "    frames = []  # To hold the cropped frames\n",
    "    \n",
    "\n",
    "    # Prepare for saving cropped video\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing RTSP stream from {rtsp_url}\")\n",
    "\n",
    "    while True:\n",
    "        # Capture a batch of frames\n",
    "        batch_frames = []\n",
    "        for _ in range(num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame. Stream ended or connection lost.\")\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            batch_frames.append(frame)\n",
    "\n",
    "        if not batch_frames:\n",
    "            break\n",
    "\n",
    "        # Detect person in the first frame of the batch and define crop area\n",
    "        if fixed_crop_area is None:\n",
    "            results = model_yolo(batch_frames[0], conf=confidence_threshold, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == 0:  # PERSON_CLASS_ID = 0\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "\n",
    "            if best_box is not None:\n",
    "                h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "                # First clamp the YOLO detection\n",
    "                clamped_box = clamp_bbox(best_box, h, w)\n",
    "\n",
    "                if clamped_box is not None:\n",
    "                    x1, y1, x2, y2 = clamped_box\n",
    "\n",
    "                    PADDING = 15\n",
    "\n",
    "                    # Add padding\n",
    "                    padded_box = (\n",
    "                        x1 - PADDING,\n",
    "                        y1 - PADDING,\n",
    "                        x2 + PADDING,\n",
    "                        y2 + PADDING\n",
    "                    )\n",
    "\n",
    "                    # Clamp the padded box again\n",
    "                    fixed_crop_area = clamp_bbox(padded_box, h, w)\n",
    "\n",
    "        # Process the batch of frames (crop and save)\n",
    "        cropped_batch = []\n",
    "        for frame in batch_frames:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = fixed_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "            cropped_batch.append(cropped_frame)\n",
    "\n",
    "        # Define output video path for this 2-second segment\n",
    "        output_video_path = os.path.join(output_folder, f\"cropped_video_{frame_count // num_frames}.mp4\")\n",
    "\n",
    "        # VideoWriter to save the cropped 2-second video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec for mp4\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (cropped_batch[0].shape[1], cropped_batch[0].shape[0]))\n",
    "\n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not open video writer for {output_video_path}\")\n",
    "            cap.release()\n",
    "            return\n",
    "\n",
    "        # Write each frame in the cropped batch to the video\n",
    "        for cropped_frame in cropped_batch:\n",
    "            out.write(cropped_frame)\n",
    "\n",
    "        # Release the video writer\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Saved cropped 2-second video: {output_video_path}\")\n",
    "\n",
    "        # Run inference on the saved 2-second cropped video with MViT\n",
    "        run_mvit_inference(output_video_path, model_path)\n",
    "\n",
    "        print(\"Inference complete.\")\n",
    "\n",
    "    cap.release()\n",
    "    print(\"RTSP stream processing stopped.\")\n",
    "\n",
    "def run_mvit_inference(video_path, model_path):\n",
    "    \"\"\"\n",
    "    Loads the MViT model and performs inference on the given video.\n",
    "    \"\"\"\n",
    "    # Load the MViT model\n",
    "    model_2 = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=3)  # Adjust `num_classes` and `K` based on your setup\n",
    "    device = next(model_2.parameters()).device\n",
    "\n",
    "    # Define video transformations\n",
    "    transform = Compose([\n",
    "        Resize((256, 256)),  # Resize input to 256x256\n",
    "        CenterCrop(224),     # Crop to 224x224 after resizing\n",
    "        NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])  # Normalize to match training conditions\n",
    "    ])\n",
    "\n",
    "    # Create a dataset and dataloader for the video\n",
    "    frames_per_clip = 16  # Number of frames per clip (or adjust as needed)\n",
    "    test_dataset = FlatVideoDataset(video_path, transform=transform, frames_per_clip=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "    # Perform inference\n",
    "    video_predictions = run_inference(test_loader, model_2, device)\n",
    "\n",
    "    # Print the predictions for each cropped frame\n",
    "    idx_to_class_name = {\n",
    "        0: \"BicepsCurls\", 1: \"FrontRaises\", 2: \"HammerCurls\", 3: \"LateralRaise\", \n",
    "        4: \"UprightRows\", 5: \"arm_circles\", 6: \"bb_military_press\", 7: \"dumbbell_chest_press\", \n",
    "        8: \"dumbbell_incline_chest_press\", 9: \"dumbbell_lunges\", 10: \"dumbbell_reverse_flys\", \n",
    "        11: \"ez_bb_curls\", 12: \"kb_gobletsquats\", 13: \"kb_ohpress\", 14: \"kb_swings\", \n",
    "        15: \"kettlebell_goodmorning\", 16: \"seated_dumbbell_shoulderpess\", 17: \"singlearm_dumbbell_rows\", \n",
    "        18: \"skull_crushers\", 19: \"spider_curls\", 20: \"tricep_dips\"\n",
    "    }\n",
    "\n",
    "    for path, prediction_idx in video_predictions.items():\n",
    "        predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "        print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "# ============================\n",
    "# Dataset and Inference\n",
    "# ============================\n",
    "\n",
    "class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        if os.path.isfile(self.root_dir) and self.root_dir.lower().endswith(\".mp4\"):\n",
    "            # Single video file\n",
    "            self.video_paths.append(self.root_dir)\n",
    "        elif os.path.isdir(self.root_dir):\n",
    "            # Directory of videos\n",
    "            for fname in os.listdir(self.root_dir):\n",
    "                if fname.lower().endswith(\".mp4\"):\n",
    "                    self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "        else:\n",
    "            raise ValueError(f\"{self.root_dir} is neither a .mp4 file nor a directory\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = -1  # Dummy label\n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "def run_inference(test_loader, model, device):\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Main Execution Logic\n",
    "# ============================\n",
    "\n",
    "# Define RTSP stream URL and output folder path\n",
    "rtsp_url = 'rtsp://127.0.0.1:8554/test'  # Replace with your actual RTSP URL\n",
    "output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'  # Folder to save cropped frames if needed\n",
    "\n",
    "# 1. Detect and Crop Person from RTSP Stream\n",
    "detect_and_crop_person_from_rtsp(rtsp_url, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RTSP stream from rtsp://127.0.0.1:8554/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61cebf4d3b80] Invalid level prefix\n",
      "[h264 @ 0x61cebf4d3b80] error while decoding MB 68 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s]\n",
      "[h264 @ 0x61ceb342f080] corrupted macroblock 52 54 (total_coeff=-1)\n",
      "[h264 @ 0x61ceb342f080] error while decoding MB 52 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61ceafcd3a40] Invalid level prefix\n",
      "[h264 @ 0x61ceafcd3a40] error while decoding MB 49 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61ced5c14900] corrupted macroblock 118 62 (total_coeff=-1)\n",
      "[h264 @ 0x61ced5c14900] error while decoding MB 118 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.44it/s]\n",
      "[h264 @ 0x61cee68170c0] corrupted macroblock 58 25 (total_coeff=-1)\n",
      "[h264 @ 0x61cee68170c0] error while decoding MB 58 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61ceafd35280] Invalid level prefix\n",
      "[h264 @ 0x61ceafd35280] error while decoding MB 18 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.64it/s]\n",
      "[h264 @ 0x61cef8bb0ac0] Invalid level prefix\n",
      "[h264 @ 0x61cef8bb0ac0] error while decoding MB 52 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.34it/s]\n",
      "[h264 @ 0x61cee68170c0] corrupted macroblock 41 31 (total_coeff=-1)\n",
      "[h264 @ 0x61cee68170c0] error while decoding MB 41 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.61it/s]\n",
      "[h264 @ 0x61cef8bb0ac0] Invalid level prefix\n",
      "[h264 @ 0x61cef8bb0ac0] error while decoding MB 35 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.48it/s]\n",
      "[h264 @ 0x61ceafd2e9c0] Invalid level prefix\n",
      "[h264 @ 0x61ceafd2e9c0] error while decoding MB 50 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61ceb342f080] Invalid level prefix\n",
      "[h264 @ 0x61ceb342f080] error while decoding MB 57 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.67it/s]\n",
      "[h264 @ 0x61ceb12dab40] Invalid level prefix\n",
      "[h264 @ 0x61ceb12dab40] error while decoding MB 52 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.57it/s]\n",
      "[h264 @ 0x61ced0a418c0] corrupted macroblock 30 51 (total_coeff=-1)\n",
      "[h264 @ 0x61ced0a418c0] error while decoding MB 30 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.61it/s]\n",
      "[h264 @ 0x61ceafd2e9c0] corrupted macroblock 34 37 (total_coeff=-1)\n",
      "[h264 @ 0x61ceafd2e9c0] error while decoding MB 34 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.14it/s]\n",
      "[h264 @ 0x61cef8bb0ac0] Invalid level prefix\n",
      "[h264 @ 0x61cef8bb0ac0] error while decoding MB 28 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61ceb12d2580] corrupted macroblock 38 19 (total_coeff=-1)\n",
      "[h264 @ 0x61ceb12d2580] error while decoding MB 38 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x61cee68170c0] corrupted macroblock 31 43 (total_coeff=-1)\n",
      "[h264 @ 0x61cee68170c0] error while decoding MB 31 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.70it/s]\n",
      "[h264 @ 0x61ceb342f080] corrupted macroblock 51 25 (total_coeff=-1)\n",
      "[h264 @ 0x61ceb342f080] error while decoding MB 51 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([11], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: db_rows\n",
      "Inference complete.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.72it/s]\n",
      "[h264 @ 0x61cee677c4c0] Invalid level prefix\n",
      "[h264 @ 0x61cee677c4c0] error while decoding MB 107 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Failed to grab frame. Stream ended or connection lost.\n",
      "Saved cropped 2-second video: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames/cropped_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([1], device='cuda:0')\n",
      "File: cropped_video.mp4                                  -> Predicted Class: FrontRaises\n",
      "Inference complete.\n",
      "Failed to grab frame. Stream ended or connection lost.\n",
      "RTSP stream processing stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "import time\n",
    "from camera import CameraManager, create_camera_configs_from_ips\n",
    "\n",
    "def clamp_bbox(bbox, h, w):\n",
    "    \"\"\"\n",
    "    Clamp bounding box coordinates to ensure they are within image boundaries.\n",
    "\n",
    "    Args:\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2)\n",
    "        h (int): Image height\n",
    "        w (int): Image width\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: Clamped bounding box coordinates (x1, y1, x2, y2) or None if invalid\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    x1 = max(0, min(w - 1, int(x1)))\n",
    "    y1 = max(0, min(h - 1, int(y1)))\n",
    "    x2 = max(0, min(w,     int(x2)))\n",
    "    y2 = max(0, min(h,     int(y2)))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def detect_and_crop_person_from_rtsp(rtsp_url, output_folder, confidence_threshold=0.5, duration_seconds=2, model_path=None):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, crops all frames of the RTSP stream to that consistent area,\n",
    "    saves a 2-second cropped video to a specified output folder, and then performs inference using the MViT model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_yolo = YOLO('yolov8n.pt')  # Load YOLOv8 model for person detection\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Open RTSP stream\n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open RTSP stream {rtsp_url}\")\n",
    "        return\n",
    "\n",
    "    # Frame rate (fps) of the video stream\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        print(\"Error: Could not retrieve FPS from the RTSP stream.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the number of frames for 2 seconds\n",
    "    num_frames = int(fps * duration_seconds)\n",
    "\n",
    "    frame_count = 0\n",
    "    fixed_crop_area = None\n",
    "    frames = []  # To hold the cropped frames\n",
    "\n",
    "    # Prepare for saving cropped video\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing RTSP stream from {rtsp_url}\")\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Capture a batch of frames\n",
    "        batch_frames = []\n",
    "        for _ in range(num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame. Stream ended or connection lost.\")\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            batch_frames.append(frame)\n",
    "\n",
    "        if not batch_frames:\n",
    "            break\n",
    "\n",
    "        # Detect person in the first frame of the batch and define crop area\n",
    "        if fixed_crop_area is None:\n",
    "            results = model_yolo(batch_frames[0], conf=confidence_threshold, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == 0:  # PERSON_CLASS_ID = 0\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "\n",
    "            if best_box is not None:\n",
    "                h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "                # First clamp the YOLO detection\n",
    "                clamped_box = clamp_bbox(best_box, h, w)\n",
    "\n",
    "                if clamped_box is not None:\n",
    "                    x1, y1, x2, y2 = clamped_box\n",
    "\n",
    "                    PADDING = 15\n",
    "\n",
    "                    # Add padding\n",
    "                    padded_box = (\n",
    "                        x1 - PADDING,\n",
    "                        y1 - PADDING,\n",
    "                        x2 + PADDING,\n",
    "                        y2 + PADDING\n",
    "                    )\n",
    "\n",
    "                    # Clamp the padded box again\n",
    "                    fixed_crop_area = clamp_bbox(padded_box, h, w)\n",
    "\n",
    "\n",
    "        # Process the batch of frames (crop and save)\n",
    "        cropped_batch = []\n",
    "        for frame in batch_frames:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = fixed_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "            cropped_batch.append(cropped_frame)\n",
    "\n",
    "        # Define output video path for this 2-second segment\n",
    "        # output_video_path = os.path.join(output_folder, f\"cropped_video_{frame_count // num_frames}.mp4\")\n",
    "        output_video_path = os.path.join(output_folder, \"cropped_video.mp4\")\n",
    "\n",
    "        # VideoWriter to save the cropped 2-second video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec for mp4\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (cropped_batch[0].shape[1], cropped_batch[0].shape[0]))\n",
    "\n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not open video writer for {output_video_path}\")\n",
    "            cap.release()\n",
    "            return\n",
    "\n",
    "        # Write each frame in the cropped batch to the video\n",
    "        for cropped_frame in cropped_batch:\n",
    "            out.write(cropped_frame)\n",
    "\n",
    "        # Release the video writer\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Saved cropped 2-second video: {output_video_path}\")\n",
    "\n",
    "        # Run inference on the saved 2-second cropped video with MViT\n",
    "        run_mvit_inference(output_video_path, model_path)\n",
    "\n",
    "        print(\"Inference complete.\")\n",
    "\n",
    "    cap.release()\n",
    "    print(\"RTSP stream processing stopped.\")\n",
    "\n",
    "def run_mvit_inference(video_path, model_path):\n",
    "    \"\"\"\n",
    "    Loads the MViT model and performs inference on the given video.\n",
    "    \"\"\"\n",
    "    # Load the MViT model\n",
    "    model_2 = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=3)  # Adjust `num_classes` and `K` based on your setup\n",
    "    device = next(model_2.parameters()).device\n",
    "\n",
    "    # Define video transformations\n",
    "    transform = Compose([\n",
    "        Resize((256, 256)),  # Resize input to 256x256\n",
    "        CenterCrop(224),     # Crop to 224x224 after resizing\n",
    "        NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])  # Normalize to match training conditions\n",
    "    ])\n",
    "\n",
    "    # Create a dataset and dataloader for the video\n",
    "    frames_per_clip = 16  # Number of frames per clip (or adjust as needed)\n",
    "    test_dataset = FlatVideoDataset(video_path, transform=transform, frames_per_clip=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "    # Perform inference\n",
    "    video_predictions = run_inference(test_loader, model_2, device)\n",
    "\n",
    "    # -------------------- NEW: AUTO-LOAD TRAINING CLASS NAMES --------------------\n",
    "\n",
    "    train_root = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Dataset_30Classes_Cam107-18_SPLIT/train\"\n",
    "\n",
    "    # Auto-read folder names in alphabetical order — EXACT training class order\n",
    "    class_names = sorted(os.listdir(train_root))\n",
    "\n",
    "    # Map index → class name automatically\n",
    "    idx_to_class_name = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "    for path, prediction_idx in video_predictions.items():\n",
    "        predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "        print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "# ============================\n",
    "# Dataset and Inference\n",
    "# ============================\n",
    "\n",
    "class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        if os.path.isfile(self.root_dir) and self.root_dir.lower().endswith(\".mp4\"):\n",
    "            # Single video file\n",
    "            self.video_paths.append(self.root_dir)\n",
    "        elif os.path.isdir(self.root_dir):\n",
    "            # Directory of videos\n",
    "            for fname in os.listdir(self.root_dir):\n",
    "                if fname.lower().endswith(\".mp4\"):\n",
    "                    self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "        else:\n",
    "            raise ValueError(f\"{self.root_dir} is neither a .mp4 file nor a directory\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = -1  # Dummy label\n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "def run_inference(test_loader, model, device):\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Main Execution Logic\n",
    "# ============================\n",
    "\n",
    "# Define RTSP stream URL and output folder path\n",
    "rtsp_url = 'rtsp://127.0.0.1:8554/test'  # Replace with your actual RTSP URL\n",
    "output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'  # Folder to save cropped frames if needed\n",
    "\n",
    "# 1. Detect and Crop Person from RTSP Stream\n",
    "detect_and_crop_person_from_rtsp(rtsp_url, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab9c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0398d0e",
   "metadata": {},
   "source": [
    "# Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4ec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 videos to process.\n",
      "--------------------------------------------------\n",
      "Processing output_000.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_000.mp4:   2%|▏         | 1/50 [00:00<00:26,  1.85it/s][NULL @ 0x5d8abe039040] Invalid NAL unit size (25525 > 21639).\n",
      "[NULL @ 0x5d8abe039040] missing picture in access unit with size 21643\n",
      "[h264 @ 0x5d8abdfdbb80] Invalid NAL unit size (25525 > 21639).\n",
      "[h264 @ 0x5d8abdfdbb80] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0d4080] stream 0, offset 0x6ff40: partial file\n",
      "Cropping output_000.mp4:  32%|███▏      | 16/50 [00:00<00:01, 28.72it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0d4080] stream 0, offset 0x71429: partial file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_000.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_001.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_001.mp4:   0%|          | 0/50 [00:00<?, ?it/s][NULL @ 0x5d8ad4db9940] Invalid NAL unit size (12542 > 12163).\n",
      "[NULL @ 0x5d8ad4db9940] missing picture in access unit with size 12167\n",
      "[h264 @ 0x5d8abe5dda80] Invalid NAL unit size (12542 > 12163).\n",
      "[h264 @ 0x5d8abe5dda80] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe08c940] stream 0, offset 0x69a37: partial file\n",
      "Cropping output_001.mp4:  36%|███▌      | 18/50 [00:00<00:00, 319.43it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe08c940] stream 0, offset 0x6bda7: partial file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_001.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_002.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_002.mp4:   0%|          | 0/50 [00:00<?, ?it/s][NULL @ 0x5d8abe3f8700] Invalid NAL unit size (10590 > 3595).\n",
      "[NULL @ 0x5d8abe3f8700] missing picture in access unit with size 3599\n",
      "[h264 @ 0x5d8abfd6b800] Invalid NAL unit size (10590 > 3595).\n",
      "[h264 @ 0x5d8abfd6b800] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0c2400] stream 0, offset 0x643a1: partial file\n",
      "Cropping output_002.mp4:  34%|███▍      | 17/50 [00:00<00:00, 285.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_002.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_003.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_003.mp4:   0%|          | 0/50 [00:00<?, ?it/s][NULL @ 0x5d8abe0feec0] Invalid NAL unit size (34639 > 11440).\n",
      "[NULL @ 0x5d8abe0feec0] missing picture in access unit with size 11444\n",
      "[h264 @ 0x5d8abe3f1380] Invalid NAL unit size (34639 > 11440).\n",
      "[h264 @ 0x5d8abe3f1380] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0f7700] stream 0, offset 0x6741e: partial file\n",
      "Cropping output_003.mp4:  32%|███▏      | 16/50 [00:00<00:00, 257.97it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0f7700] stream 0, offset 0x69785: partial file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_003.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_004.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_004.mp4:   0%|          | 0/50 [00:00<?, ?it/s][NULL @ 0x5d8abe327600] Invalid NAL unit size (9937 > 276).\n",
      "[NULL @ 0x5d8abe327600] missing picture in access unit with size 280\n",
      "[h264 @ 0x5d8abf9a9240] Invalid NAL unit size (9937 > 276).\n",
      "[h264 @ 0x5d8abf9a9240] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe6a1040] stream 0, offset 0x6c607: partial file\n",
      "Cropping output_004.mp4:  36%|███▌      | 18/50 [00:00<00:00, 307.36it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe6a1040] stream 0, offset 0x6e6e6: partial file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_004.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_005.mp4 with 50 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_005.mp4:   0%|          | 0/50 [00:00<?, ?it/s][NULL @ 0x5d8abff01400] Invalid NAL unit size (10265 > 8865).\n",
      "[NULL @ 0x5d8abff01400] missing picture in access unit with size 8869\n",
      "[h264 @ 0x5d8abe648900] Invalid NAL unit size (10265 > 8865).\n",
      "[h264 @ 0x5d8abe648900] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe51fa40] stream 0, offset 0x6844b: partial file\n",
      "Cropping output_005.mp4:  34%|███▍      | 17/50 [00:00<00:00, 314.19it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe51fa40] stream 0, offset 0x6ae9d: partial file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_005.mp4\n",
      "--------------------------------------------------\n",
      "Processing output_006.mp4 with 45 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping output_006.mp4:   0%|          | 0/45 [00:00<?, ?it/s][NULL @ 0x5d8ac006c440] Invalid NAL unit size (10889 > 10136).\n",
      "[NULL @ 0x5d8ac006c440] missing picture in access unit with size 10140\n",
      "[h264 @ 0x5d8abfe84d00] Invalid NAL unit size (10889 > 10136).\n",
      "[h264 @ 0x5d8abfe84d00] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0538c0] stream 0, offset 0x6820d: partial file\n",
      "Cropping output_006.mp4:  40%|████      | 18/45 [00:00<00:00, 335.61it/s][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5d8abe0538c0] stream 0, offset 0x6c399: partial file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Cropped video saved to: /home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/output_006.mp4\n",
      "\n",
      "Batch processing complete for all videos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import glob # Import the glob module\n",
    "from ultralytics import YOLO \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "YOLO_MODEL = 'yolov8n.pt'  \n",
    "CONFIDENCE_THRESHOLD = 0.5 \n",
    "PADDING = 10               \n",
    "PERSON_CLASS_ID = 0        \n",
    "# Define accepted video extensions\n",
    "VIDEO_EXTENSIONS = ('*.mp4', '*.avi', '*.mov', '*.mkv')\n",
    "\n",
    "def detect_and_crop_person(input_video_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, and crops all frames of the video to that consistent area.\n",
    "    \"\"\"\n",
    "    # 1. Initialize YOLOv8 Model (loaded inside function for clean scope per video process)\n",
    "    try:\n",
    "        # Suppress ultralytics output during batch processing for cleaner console\n",
    "        model = YOLO(YOLO_MODEL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model. Ensure 'ultralytics' is installed: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Initialize Video Reader\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    out = None\n",
    "    fixed_crop_area = None # Stores the fixed [x1, y1, x2, y2] area (tuple of ints)\n",
    "    output_dims = None     # Stores the final (width, height) tuple for the VideoWriter\n",
    "\n",
    "    print(f\"Processing {os.path.basename(input_video_path)} with {total_frames} frames...\")\n",
    "\n",
    "    # 3. Process Frames\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Cropping {os.path.basename(input_video_path)}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        97.67\n",
    "        current_crop_area = None\n",
    "\n",
    "        if fixed_crop_area is None:\n",
    "            # Try to find a person to establish the fixed crop area\n",
    "            results = model(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == PERSON_CLASS_ID:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "            \n",
    "            if best_box is not None:\n",
    "                # Set the fixed crop area based on the first detection with padding and clamping\n",
    "                x1, y1, x2, y2 = best_box\n",
    "                fixed_crop_area = (\n",
    "                    max(0, x1 - PADDING),\n",
    "                    max(0, y1 - PADDING),\n",
    "                    min(frame_width, x2 + PADDING),\n",
    "                    min(frame_height, y2 + PADDING)\n",
    "                )\n",
    "                current_crop_area = fixed_crop_area\n",
    "                # Calculate the output dimensions once (W, H)\n",
    "                crop_width = fixed_crop_area[2] - fixed_crop_area[0]\n",
    "                crop_height = fixed_crop_area[3] - fixed_crop_area[1]\n",
    "                output_dims = (crop_width, crop_height)\n",
    "        else:\n",
    "            # Use the already established fixed crop area for all subsequent frames\n",
    "            current_crop_area = fixed_crop_area\n",
    "\n",
    "        # --- Frame Cropping and Writing Logic ---\n",
    "        frame_to_write = None\n",
    "\n",
    "        if current_crop_area is not None:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = current_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "            frame_to_write = cropped_frame\n",
    "\n",
    "            # Initialize VideoWriter if not already done\n",
    "            if out is None and output_dims is not None:\n",
    "                # Ensure width/height are positive for the writer to work\n",
    "                if output_dims[0] > 0 and output_dims[1] > 0:\n",
    "                    out = cv2.VideoWriter(output_video_path, fourcc, fps, output_dims)\n",
    "                else:\n",
    "                    print(f\"\\nWarning: Invalid output dimensions {output_dims} for {os.path.basename(input_video_path)}. Skipping video.\")\n",
    "                    break # Exit the frame loop for this video\n",
    "\n",
    "        if out is not None:\n",
    "            if frame_to_write is None or frame_to_write.size == 0:\n",
    "                # If no valid crop occurred, write a black frame of the defined size\n",
    "                # Use output_dims[1] for H, output_dims[0] for W in shape tuple\n",
    "                frame_to_write = np.zeros((output_dims[1], output_dims[0], 3), dtype=np.uint8)\n",
    "            \n",
    "            # This resize is a fallback but shouldn't be strictly necessary with fixed area logic\n",
    "            if frame_to_write.shape[0] != output_dims[1] or frame_to_write.shape[1] != output_dims[0]:\n",
    "                frame_to_write = cv2.resize(frame_to_write, output_dims)\n",
    "            \n",
    "            out.write(frame_to_write)\n",
    "        \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "        print(f\"\\nFinished processing. Cropped video saved to: {output_video_path}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Could not finalize video processing for {os.path.basename(input_video_path)} (perhaps no person was detected in the whole video). No output file created.\")\n",
    "\n",
    "\n",
    "# --- Batch Processing Logic ---\n",
    "if __name__ == '__main__':\n",
    "    # Define your input and output directories\n",
    "    INPUT_DIR = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set\"\n",
    "    OUTPUT_DIR = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True) #\n",
    "\n",
    "    # Find all video files in the input directory\n",
    "    video_files = []\n",
    "    for ext in VIDEO_EXTENSIONS:\n",
    "        video_files.extend(glob.glob(os.path.join(INPUT_DIR, ext))) #\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found with extensions {VIDEO_EXTENSIONS} in {INPUT_DIR}\")\n",
    "    else:\n",
    "        print(f\"Found {len(video_files)} videos to process.\")\n",
    "\n",
    "    # Process each video file found\n",
    "    for input_path in sorted(video_files): # Sorting ensures consistent order\n",
    "        filename = os.path.basename(input_path)\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        detect_and_crop_person(input_path, output_path)\n",
    "\n",
    "    print(\"\\nBatch processing complete for all videos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from decord import VideoReader, cpu\n",
    "from torch.utils.data import Dataset\n",
    "# Ensure config, tqdm, and other imports are available\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "\n",
    "\n",
    "class FlatVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads videos directly from a single flat folder, assigning a dummy label (-1).\n",
    "    Used for inference where true labels are unknown or unnecessary.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        print(\"########### BUILD INDEX TRACKING (FLAT DIRECTORY) ###########\")\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if fname.lower().endswith(\".mp4\"):\n",
    "                self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "        print(f\" |Num videos found: {len(self.video_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        # Assign a dummy label, as the ground truth is unknown for inference\n",
    "        label = -1 \n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            # --- Frame Indexing Logic (Keep the same) ---\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()          # (T,H,W,C)\n",
    "\n",
    "            # Fix for grayscale videos\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            # (C,T,H,W), normalize to [0, 1]\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            # Return frames, dummy label, and the original file path for reference\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            # Try next video (avoid infinite recursion if dataset has 0 length)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def run_infernce_flat(test_loader, model):\n",
    "    # Lists to store predicted class labels and the video file paths\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "\n",
    "    # Get the device from the model\n",
    "    device = next(model.parameters()).device \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # The DataLoader now yields 3 items: inputs, dummy_labels, paths\n",
    "        for inputs, _, paths in tqdm(test_loader, desc='Inference'):\n",
    "            \n",
    "            inputs = inputs.to(device) \n",
    "            \n",
    "            # Perform the forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class index\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Store the predictions and paths\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    # Return a dictionary mapping file path to predicted class index\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "# --- IMPORTANT: Ensure K and NormalizeVideo are defined/imported ---\n",
    "# K must match your training (e.g., K=3)\n",
    "K = 3 \n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a8bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### BUILD INDEX TRACKING (FLAT DIRECTORY) ###########\n",
      " |Num videos found: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 2/2 [00:00<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "################# INFERENCE RESULTS #################\n",
      "File: antotest_114_clip01_00026_00370_seg004.mp4         -> Predicted Class: FrontRaises\n",
      "File: antotest_114_clip01_00026_00370_seg005.mp4         -> Predicted Class: FrontRaises\n",
      "File: antotest_114_clip01_00026_00370_seg003.mp4         -> Predicted Class: FrontRaises\n",
      "File: antotest_114_clip01_00026_00370_seg006.mp4         -> Predicted Class: FrontRaises\n",
      "File: antotest_114_clip01_00026_00370_seg001.mp4         -> Predicted Class: FrontRaises\n",
      "File: antotest_114_clip01_00026_00370_seg002.mp4         -> Predicted Class: FrontRaises\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing the videos you want to test\n",
    "flat_test_folder = \"/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/anto_proper_test_set/anto_yolo\" # <-- CHANGE THIS PATH!\n",
    "\n",
    "# 1. Load Model (Using the corrected function signature)\n",
    "model = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=K)\n",
    "\n",
    "# Set the device variable for use in DataLoader\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 2. Define Transforms (Keep the same)\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    CenterCrop(224),\n",
    "    NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Load Flat Dataset\n",
    "# frames_per_clip must also be defined (e.g., frames_per_clip=16)\n",
    "frames_per_clip = 16 \n",
    "test_dataset = FlatVideoDataset(flat_test_folder, transform=transform, frames_per_clip=frames_per_clip)\n",
    "\n",
    "# 4. Create DataLoader\n",
    "test_loader   = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "# 5. Run Inference\n",
    "video_predictions = run_infernce_flat(test_loader, model)\n",
    "\n",
    "# 6. Output Results\n",
    "print(\"\\n\\n################# INFERENCE RESULTS #################\")\n",
    "\n",
    "# Create the index-to-class mapping (Invert your training map if possible)\n",
    "idx_to_class_name = {\n",
    "    0: \"BicepsCurls\", 1: \"FrontRaises\", 2: \"HammerCurls\", 3: \"LateralRaise\", \n",
    "    4: \"UprightRows\", 5: \"arm_circles\", 6: \"bb_military_press\", 7: \"dumbbell_chest_press\", \n",
    "    8: \"dumbbell_incline_chest_press\", 9: \"dumbbell_lunges\", 10: \"dumbbell_reverse_flys\", \n",
    "    11: \"ez_bb_curls\", 12: \"kb_gobletsquats\", 13: \"kb_ohpress\", 14: \"kb_swings\", \n",
    "    15: \"kettlebell_goodmorning\", 16: \"seated_dumbbell_shoulderpess\", 17: \"singlearm_dumbbell_rows\", \n",
    "    18: \"skull_crushers\", 19: \"spider_curls\", 20: \"tricep_dips\"\n",
    "}\n",
    "\n",
    "for path, prediction_idx in video_predictions.items():\n",
    "    \n",
    "    # Look up the corresponding class name\n",
    "    predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "    \n",
    "    # Print the result using the class name\n",
    "    print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "    \n",
    "    \n",
    "print(\"#####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea76db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "_transforms_video is available\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Fallback _transforms_video not available\n",
      "Using MLflow Run ID: 840f3d39813c41ba9880859c83a82b01\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "Loading model from MLflow: runs:/840f3d39813c41ba9880859c83a82b01/best_model\n",
      "Downloading artifacts:   0%|                              | 0/1 [00:00<?, ?it/s]\n",
      "Downloading artifacts: 100%|█████████████████████| 6/6 [00:00<00:00, 154.44it/s]\n",
      "########### BUILD INDEX TRACKING (TRAINING ORDER) ###########\n",
      " |Test classes: ['BicepsCurls', 'FrontRaises', 'HammerCurls', 'LateralRaise', 'arm_circles', 'dumbbell_chest_press', 'dumbbell_incline_chest_press', 'dumbbell_lunges', 'kb_gobletsquats', 'kb_ohpress', 'kb_swings', 'kettlebell_goodmorning', 'seated_dumbbell_shoulderpess', 'singlearm_dumbbell_rows']\n",
      " |Using training mapping: {'BicepsCurls': 0, 'FrontRaises': 1, 'HammerCurls': 2, 'LateralRaise': 3, 'UprightRows': 4, 'arm_circles': 5, 'bb_military_press': 6, 'dumbbell_chest_press': 7, 'dumbbell_incline_chest_press': 8, 'dumbbell_lunges': 9, 'dumbbell_reverse_flys': 10, 'ez_bb_curls': 11, 'kb_gobletsquats': 12, 'kb_ohpress': 13, 'kb_swings': 14, 'kettlebell_goodmorning': 15, 'seated_dumbbell_shoulderpess': 16, 'singlearm_dumbbell_rows': 17, 'skull_crushers': 18, 'spider_curls': 19, 'tricep_dips': 20}\n",
      "Inference: 100%|██████████████████████████████████| 5/5 [00:01<00:00,  4.00it/s]\n",
      "\n",
      "Overall Test Accuracy: 85.00%\n",
      "\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                 BicepsCurls       1.00      1.00      1.00         1\n",
      "                 FrontRaises       1.00      1.00      1.00         1\n",
      "                 HammerCurls       1.00      1.00      1.00         1\n",
      "                LateralRaise       1.00      1.00      1.00         1\n",
      "                 arm_circles       1.00      1.00      1.00         1\n",
      "        dumbbell_chest_press       1.00      0.50      0.67         2\n",
      "dumbbell_incline_chest_press       0.50      0.50      0.50         2\n",
      "             dumbbell_lunges       1.00      1.00      1.00         1\n",
      "             kb_gobletsquats       1.00      0.50      0.67         2\n",
      "                  kb_ohpress       1.00      1.00      1.00         2\n",
      "                   kb_swings       0.67      1.00      0.80         2\n",
      "      kettlebell_goodmorning       1.00      1.00      1.00         2\n",
      "seated_dumbbell_shoulderpess       0.50      1.00      0.67         1\n",
      "     singlearm_dumbbell_rows       1.00      1.00      1.00         1\n",
      "\n",
      "                    accuracy                           0.85        20\n",
      "                   macro avg       0.90      0.89      0.88        20\n",
      "                weighted avg       0.89      0.85      0.85        20\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/smartan5070/Downloads/SlowfastTrainer-main/test_MViT.py --run_id '840f3d39813c41ba9880859c83a82b01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd4f75",
   "metadata": {},
   "source": [
    "# RTSP stream from camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e808bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added camera cam_1: Camera 1\n",
      "Started camera cam_1\n",
      "Started 1/1 cameras\n",
      "Camera started!\n",
      "Streaming from 192.168.0.110... Press Q to exit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 314\u001b[0m\n\u001b[1;32m    311\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Folder to save cropped frames if needed\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# 1. Detect and Crop Person from RTSP Stream\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mdetect_and_crop_person_from_rtsp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrtsp_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m, in \u001b[0;36mdetect_and_crop_person_from_rtsp\u001b[0;34m(rtsp_ip, output_folder, confidence_threshold, duration_seconds, model_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     frames \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mget_frames()\n\u001b[0;32m---> 72\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcam_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "# from decord import VideoReader, cpu\n",
    "# from tqdm import tqdm\n",
    "# from ultralytics import YOLO\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "# import time\n",
    "# from camera import CameraManager, create_camera_configs_from_ips\n",
    "\n",
    "# def clamp_bbox(bbox, h, w):\n",
    "#     \"\"\"\n",
    "#     Clamp bounding box coordinates to ensure they are within image boundaries.\n",
    "\n",
    "#     Args:\n",
    "#         bbox (tuple): Bounding box coordinates (x1, y1, x2, y2)\n",
    "#         h (int): Image height\n",
    "#         w (int): Image width\n",
    "\n",
    "#     Returns:\n",
    "#         tuple or None: Clamped bounding box coordinates (x1, y1, x2, y2) or None if invalid\n",
    "#     \"\"\"\n",
    "#     x1, y1, x2, y2 = bbox\n",
    "\n",
    "#     x1 = max(0, min(w - 1, int(x1)))\n",
    "#     y1 = max(0, min(h - 1, int(y1)))\n",
    "#     x2 = max(0, min(w,     int(x2)))\n",
    "#     y2 = max(0, min(h,     int(y2)))\n",
    "\n",
    "#     if x2 <= x1 or y2 <= y1:\n",
    "#         return None\n",
    "\n",
    "#     return x1, y1, x2, y2\n",
    "\n",
    "# def detect_and_crop_person_from_rtsp(rtsp_ip, output_folder,\n",
    "#                                      confidence_threshold=0.5,\n",
    "#                                      duration_seconds=2,\n",
    "#                                      model_path=None):\n",
    "\n",
    "#     # Load YOLO\n",
    "#     try:\n",
    "#         model_yolo = YOLO(\"yolov8n.pt\")\n",
    "#     except Exception as e:\n",
    "#         print(\"Failed to load YOLO:\", e)\n",
    "#         return\n",
    "\n",
    "#     # ---- SETUP CAMERA USING camera.py ----\n",
    "#     camera_configs = create_camera_configs_from_ips([rtsp_ip])\n",
    "#     manager = CameraManager(display_width=640, display_height=480)\n",
    "#     manager.add_camera(\"cam_1\", camera_configs[0])\n",
    "#     manager.start_all_cameras()\n",
    "#     print(\"Camera started!\")\n",
    "\n",
    "#     # FPS assumption\n",
    "#     fps = 25\n",
    "#     frames_needed = int(fps * duration_seconds)\n",
    "\n",
    "#     fixed_crop_area = None\n",
    "#     buffer_frames = []\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     print(f\"Streaming from {rtsp_ip}... Press Q to exit.\")\n",
    "\n",
    "#     while True:\n",
    "\n",
    "#         frames = manager.get_frames()\n",
    "#         frame = frames.get(\"cam_1\", None)\n",
    "\n",
    "#         if frame is None:\n",
    "#             continue\n",
    "\n",
    "#         # ---- SHOW LIVE CAMERA FEED ----\n",
    "#         cv2.imshow(\"Live Stream\", frame)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#         # -------------------------------\n",
    "#         # PERSON DETECTION FIRST FRAME ONLY\n",
    "#         # -------------------------------\n",
    "#         if fixed_crop_area is None:\n",
    "#             results = model_yolo(frame, conf=confidence_threshold, verbose=False)\n",
    "\n",
    "#             best_box = None\n",
    "#             max_area = 0\n",
    "\n",
    "#             for r in results:\n",
    "#                 for box in r.boxes:\n",
    "#                     if box.cls.item() == 0:  # person\n",
    "#                         x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "#                         area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "#                         if area > max_area:\n",
    "#                             best_box = (x1, y1, x2, y2)\n",
    "#                             max_area = area\n",
    "\n",
    "#             if best_box is not None:\n",
    "#                 h, w = frame.shape[:2]\n",
    "#                 fixed_crop_area = clamp_bbox(best_box, h, w)\n",
    "#                 print(\"Locked crop area:\", fixed_crop_area)\n",
    "#                 continue\n",
    "\n",
    "#         # -------------------------------\n",
    "#         # COLLECT FRAMES FOR 2 SECONDS\n",
    "#         # -------------------------------\n",
    "#         if fixed_crop_area:\n",
    "#             buffer_frames.append(frame)\n",
    "\n",
    "#         if len(buffer_frames) < frames_needed:\n",
    "#             continue\n",
    "\n",
    "#         # -------------------------------\n",
    "#         # CROP FRAMES\n",
    "#         # -------------------------------\n",
    "#         x1, y1, x2, y2 = fixed_crop_area\n",
    "#         cropped_frames = [f[y1:y2, x1:x2] for f in buffer_frames]\n",
    "\n",
    "#         output_path = os.path.join(output_folder, \"cropped_video.mp4\")\n",
    "\n",
    "#         writer = cv2.VideoWriter(\n",
    "#             output_path,\n",
    "#             cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "#             fps,\n",
    "#             (cropped_frames[0].shape[1], cropped_frames[0].shape[0])\n",
    "#         )\n",
    "\n",
    "#         for f in cropped_frames:\n",
    "#             writer.write(f)\n",
    "#         writer.release()\n",
    "\n",
    "#         print(\"Saved:\", output_path)\n",
    "\n",
    "#         # -------------------------------\n",
    "#         # RUN MViT INFERENCE\n",
    "#         # -------------------------------\n",
    "#         run_mvit_inference(output_path, model_path)\n",
    "\n",
    "#         # clear buffer for next cycle\n",
    "#         buffer_frames = []\n",
    "\n",
    "#     # END LOOP\n",
    "#     manager.stop_all_cameras()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# def run_mvit_inference(video_path, model_path):\n",
    "#     \"\"\"\n",
    "#     Loads the MViT model and performs inference on the given video.\n",
    "#     \"\"\"\n",
    "#     # Load the MViT model\n",
    "#     model_2 = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=3)  # Adjust `num_classes` and `K` based on your setup\n",
    "#     device = next(model_2.parameters()).device\n",
    "\n",
    "#     # Define video transformations\n",
    "#     transform = Compose([\n",
    "#         Resize((256, 256)),  # Resize input to 256x256\n",
    "#         CenterCrop(224),     # Crop to 224x224 after resizing\n",
    "#         NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])  # Normalize to match training conditions\n",
    "#     ])\n",
    "\n",
    "#     # Create a dataset and dataloader for the video\n",
    "#     frames_per_clip = 16  # Number of frames per clip (or adjust as needed)\n",
    "#     test_dataset = FlatVideoDataset(video_path, transform=transform, frames_per_clip=16)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "#     # Perform inference\n",
    "#     video_predictions = run_inference(test_loader, model_2, device)\n",
    "\n",
    "#     # -------------------- NEW: AUTO-LOAD TRAINING CLASS NAMES --------------------\n",
    "\n",
    "#     train_root = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Dataset_30Classes_Cam107-18_SPLIT/train\"\n",
    "\n",
    "#     # Auto-read folder names in alphabetical order — EXACT training class order\n",
    "#     class_names = sorted(os.listdir(train_root))\n",
    "\n",
    "#     # Map index → class name automatically\n",
    "#     idx_to_class_name = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "#     for path, prediction_idx in video_predictions.items():\n",
    "#         predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "#         print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "# # ============================\n",
    "# # Dataset and Inference\n",
    "# # ============================\n",
    "\n",
    "# class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.frames_per_clip = frames_per_clip\n",
    "#         self.video_paths = []\n",
    "#         self._build_index()\n",
    "\n",
    "#     def _build_index(self):\n",
    "#         if os.path.isfile(self.root_dir) and self.root_dir.lower().endswith(\".mp4\"):\n",
    "#             # Single video file\n",
    "#             self.video_paths.append(self.root_dir)\n",
    "#         elif os.path.isdir(self.root_dir):\n",
    "#             # Directory of videos\n",
    "#             for fname in os.listdir(self.root_dir):\n",
    "#                 if fname.lower().endswith(\".mp4\"):\n",
    "#                     self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "#         else:\n",
    "#             raise ValueError(f\"{self.root_dir} is neither a .mp4 file nor a directory\")\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         path = self.video_paths[idx]\n",
    "#         label = -1  # Dummy label\n",
    "        \n",
    "#         try:\n",
    "#             vr = VideoReader(path, ctx=cpu(0))\n",
    "#             total_frames = len(vr)\n",
    "\n",
    "#             if total_frames < self.frames_per_clip:\n",
    "#                 base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "#                 pad = self.frames_per_clip - total_frames\n",
    "#                 frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "#             else:\n",
    "#                 frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "#             frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "#             if frames.shape[-1] == 1:\n",
    "#                 frames = np.repeat(frames, 3, axis=-1)\n",
    "#             elif frames.shape[-1] != 3:\n",
    "#                 raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "#             frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "#             if self.transform:\n",
    "#                 frames = self.transform(frames)\n",
    "\n",
    "#             return frames, label, path \n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "#             return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "# def load_model(model_path, num_classes, K):\n",
    "    \n",
    "#     weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "#     # Load the model directly\n",
    "#     model = mvit_v1_b(weights=weights)\n",
    "\n",
    "#     # Freeze all layers\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     last_fc_layer = model.head[-1]\n",
    "#     in_features = last_fc_layer.in_features\n",
    "#     model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "#     #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "#     blocks = list(model.blocks)\n",
    "#     for block in blocks[-K:]:\n",
    "#         for p in block.parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#     # Load the state dictionary\n",
    "#     model_state_dict = torch.load(model_path)\n",
    "#     # Load the state dictionary INTO the instantiated model object\n",
    "#     model.load_state_dict(model_state_dict)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "# class NormalizeVideo(nn.Module):\n",
    "#     def __init__(self, mean, std):\n",
    "#         super().__init__()\n",
    "#         self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "#         self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "#     def forward(self, tensor):\n",
    "#         return (tensor - self.mean) / self.std\n",
    "\n",
    "# def run_inference(test_loader, model, device):\n",
    "#     all_predictions = []\n",
    "#     all_video_paths = []\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             print(f\"Predicted: {predicted}\")\n",
    "#             all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "#             all_video_paths.extend(paths)\n",
    "\n",
    "#     return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "\n",
    "# # ============================\n",
    "# # Main Execution Logic\n",
    "# # ============================\n",
    "\n",
    "# # Define RTSP stream URL and output folder path\n",
    "# rtsp_url = '192.168.0.110'  # Replace with your actual RTSP URL\n",
    "# output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'  # Folder to save cropped frames if needed\n",
    "\n",
    "# # 1. Detect and Crop Person from RTSP Stream\n",
    "# detect_and_crop_person_from_rtsp(rtsp_url, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60695e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "import time\n",
    "from camera import CameraManager, create_camera_configs_from_ips\n",
    "\n",
    "def clamp_bbox(bbox, h, w):\n",
    "    \"\"\"\n",
    "    Clamp bounding box coordinates to ensure they are within image boundaries.\n",
    "\n",
    "    Args:\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2)\n",
    "        h (int): Image height\n",
    "        w (int): Image width\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: Clamped bounding box coordinates (x1, y1, x2, y2) or None if invalid\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    x1 = max(0, min(w - 1, int(x1)))\n",
    "    y1 = max(0, min(h - 1, int(y1)))\n",
    "    x2 = max(0, min(w,     int(x2)))\n",
    "    y2 = max(0, min(h,     int(y2)))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def detect_and_crop_person_from_rtsp(rtsp_url, output_folder, confidence_threshold=0.5, duration_seconds=2, model_path=None):\n",
    "    \"\"\"\n",
    "    Detects a person in the first valid frame, sets a fixed crop area based on that \n",
    "    detection, crops all frames of the RTSP stream to that consistent area,\n",
    "    saves a 2-second cropped video to a specified output folder, and then performs inference using the MViT model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_yolo = YOLO('yolov8n.pt')  # Load YOLOv8 model for person detection\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLOv8 model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Open RTSP stream\n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open RTSP stream {rtsp_url}\")\n",
    "        return\n",
    "\n",
    "    # Frame rate (fps) of the video stream\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        print(\"Error: Could not retrieve FPS from the RTSP stream.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the number of frames for 2 seconds\n",
    "    num_frames = int(fps * duration_seconds)\n",
    "\n",
    "    frame_count = 0\n",
    "    fixed_crop_area = None\n",
    "    frames = []  # To hold the cropped frames\n",
    "\n",
    "    # Prepare for saving cropped video\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing RTSP stream from {rtsp_url}\")\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Capture a batch of frames\n",
    "        batch_frames = []\n",
    "        for _ in range(num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame. Stream ended or connection lost.\")\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            batch_frames.append(frame)\n",
    "\n",
    "        if not batch_frames:\n",
    "            break\n",
    "\n",
    "        # Detect person in the first frame of the batch and define crop area\n",
    "        if fixed_crop_area is None:\n",
    "            results = model_yolo(batch_frames[0], conf=confidence_threshold, verbose=False)\n",
    "            best_box = None\n",
    "            max_area = 0\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    if box.cls.item() == 0:  # PERSON_CLASS_ID = 0\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_box = (x1, y1, x2, y2)\n",
    "\n",
    "            if best_box is not None:\n",
    "                h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "                # First clamp the YOLO detection\n",
    "                clamped_box = clamp_bbox(best_box, h, w)\n",
    "\n",
    "                if clamped_box is not None:\n",
    "                    x1, y1, x2, y2 = clamped_box\n",
    "\n",
    "                    PADDING = 15\n",
    "\n",
    "                    # Add padding\n",
    "                    padded_box = (\n",
    "                        x1 - PADDING,\n",
    "                        y1 - PADDING,\n",
    "                        x2 + PADDING,\n",
    "                        y2 + PADDING\n",
    "                    )\n",
    "\n",
    "                    # Clamp the padded box again\n",
    "                    fixed_crop_area = clamp_bbox(padded_box, h, w)\n",
    "\n",
    "\n",
    "        # Process the batch of frames (crop and save)\n",
    "        cropped_batch = []\n",
    "        for frame in batch_frames:\n",
    "            x1_crop, y1_crop, x2_crop, y2_crop = fixed_crop_area\n",
    "            cropped_frame = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "            cropped_batch.append(cropped_frame)\n",
    "\n",
    "        # Define output video path for this 2-second segment\n",
    "        # output_video_path = os.path.join(output_folder, f\"cropped_video_{frame_count // num_frames}.mp4\")\n",
    "        output_video_path = os.path.join(output_folder, \"cropped_video.mp4\")\n",
    "\n",
    "        # VideoWriter to save the cropped 2-second video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec for mp4\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (cropped_batch[0].shape[1], cropped_batch[0].shape[0]))\n",
    "\n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not open video writer for {output_video_path}\")\n",
    "            cap.release()\n",
    "            return\n",
    "\n",
    "        # Write each frame in the cropped batch to the video\n",
    "        for cropped_frame in cropped_batch:\n",
    "            out.write(cropped_frame)\n",
    "\n",
    "        # Release the video writer\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Saved cropped 2-second video: {output_video_path}\")\n",
    "\n",
    "        # Run inference on the saved 2-second cropped video with MViT\n",
    "        run_mvit_inference(output_video_path, model_path)\n",
    "\n",
    "        print(\"Inference complete.\")\n",
    "\n",
    "    cap.release()\n",
    "    print(\"RTSP stream processing stopped.\")\n",
    "\n",
    "def run_mvit_inference(video_path, model_path):\n",
    "    \"\"\"\n",
    "    Loads the MViT model and performs inference on the given video.\n",
    "    \"\"\"\n",
    "    # Load the MViT model\n",
    "    model_2 = load_model('/home/smartan5070/Downloads/SlowfastTrainer-main/Models/Testing_2Classes_Cam10718/Testing_21_acc_98_MViT.pt', num_classes=21, K=3)  # Adjust `num_classes` and `K` based on your setup\n",
    "    device = next(model_2.parameters()).device\n",
    "\n",
    "    # Define video transformations\n",
    "    transform = Compose([\n",
    "        Resize((256, 256)),  # Resize input to 256x256\n",
    "        CenterCrop(224),     # Crop to 224x224 after resizing\n",
    "        NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])  # Normalize to match training conditions\n",
    "    ])\n",
    "\n",
    "    # Create a dataset and dataloader for the video\n",
    "    frames_per_clip = 16  # Number of frames per clip (or adjust as needed)\n",
    "    test_dataset = FlatVideoDataset(video_path, transform=transform, frames_per_clip=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "    # Perform inference\n",
    "    video_predictions = run_inference(test_loader, model_2, device)\n",
    "\n",
    "    # -------------------- NEW: AUTO-LOAD TRAINING CLASS NAMES --------------------\n",
    "\n",
    "    train_root = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Dataset_30Classes_Cam107-18_SPLIT/train\"\n",
    "\n",
    "    # Auto-read folder names in alphabetical order — EXACT training class order\n",
    "    class_names = sorted(os.listdir(train_root))\n",
    "\n",
    "    # Map index → class name automatically\n",
    "    idx_to_class_name = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "    for path, prediction_idx in video_predictions.items():\n",
    "        predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "        print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "# ============================\n",
    "# Dataset and Inference\n",
    "# ============================\n",
    "\n",
    "class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.video_paths = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        if os.path.isfile(self.root_dir) and self.root_dir.lower().endswith(\".mp4\"):\n",
    "            # Single video file\n",
    "            self.video_paths.append(self.root_dir)\n",
    "        elif os.path.isdir(self.root_dir):\n",
    "            # Directory of videos\n",
    "            for fname in os.listdir(self.root_dir):\n",
    "                if fname.lower().endswith(\".mp4\"):\n",
    "                    self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "        else:\n",
    "            raise ValueError(f\"{self.root_dir} is neither a .mp4 file nor a directory\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.video_paths[idx]\n",
    "        label = -1  # Dummy label\n",
    "        \n",
    "        try:\n",
    "            vr = VideoReader(path, ctx=cpu(0))\n",
    "            total_frames = len(vr)\n",
    "\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "                pad = self.frames_per_clip - total_frames\n",
    "                frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "            else:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "            frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "            if frames.shape[-1] == 1:\n",
    "                frames = np.repeat(frames, 3, axis=-1)\n",
    "            elif frames.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "            if self.transform:\n",
    "                frames = self.transform(frames)\n",
    "\n",
    "            return frames, label, path \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "def load_model(model_path, num_classes, K):\n",
    "    \n",
    "    weights = MViT_V1_B_Weights.DEFAULT\n",
    "    \n",
    "    # Load the model directly\n",
    "    model = mvit_v1_b(weights=weights)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    last_fc_layer = model.head[-1]\n",
    "    in_features = last_fc_layer.in_features\n",
    "    model.head[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    #  Unfreeze the last K blocks (Crucial step! Must match training setup)\n",
    "    blocks = list(model.blocks)\n",
    "    for block in blocks[-K:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model_state_dict = torch.load(model_path)\n",
    "    # Load the state dictionary INTO the instantiated model object\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "class NormalizeVideo(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "    def forward(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "def run_inference(test_loader, model, device):\n",
    "    all_predictions = []\n",
    "    all_video_paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "            all_video_paths.extend(paths)\n",
    "\n",
    "    return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "\n",
    "# # ============================\n",
    "# # Main Execution Logic\n",
    "# # ============================\n",
    "\n",
    "# # Define RTSP stream URL and output folder path\n",
    "# rtsp_url = 'rtsp://127.0.0.1:8554/test'  # Replace with your actual RTSP URL\n",
    "# output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'  # Folder to save cropped frames if needed\n",
    "\n",
    "# # 1. Detect and Crop Person from RTSP Stream\n",
    "# detect_and_crop_person_from_rtsp(rtsp_url, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a16107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added camera cam_1: Camera 1\n",
      "Created 1 cameras\n",
      "  cam_1: Camera 1 - rtsp://admin:admin%40123@192.168.0.110:554/Streaming/Channels/110/\n",
      "Started camera cam_1\n",
      "Started 1/1 cameras\n",
      "Camera cam_1 pipeline started at 1764844511056708712 ns (wall clock)\n",
      "All cameras stopped\n"
     ]
    }
   ],
   "source": [
    "from camera import CameraManager, create_camera_configs_from_ips\n",
    "import cv2\n",
    "\n",
    "camera_ips = [\n",
    "    \"192.168.0.110\"\n",
    "]\n",
    "\n",
    "# Create configurations\n",
    "camera_configs = create_camera_configs_from_ips(camera_ips)\n",
    "\n",
    "# Initialize camera manager\n",
    "manager = CameraManager(display_width=640, display_height=480)\n",
    "\n",
    "# Add cameras\n",
    "for i, config in enumerate(camera_configs):\n",
    "    camera_id = f\"cam_{i+1}\"\n",
    "    manager.add_camera(camera_id, config)\n",
    "\n",
    "print(f\"Created {manager.get_camera_count()} cameras\")\n",
    "for camera_id in manager.get_camera_ids():\n",
    "    camera = manager.get_camera(camera_id)\n",
    "    print(f\"  {camera_id}: {camera.get_name()} - {camera.get_config()['url']}\")\n",
    "\n",
    "\n",
    "# ⭐ START THE CAMERAS ⭐\n",
    "manager.start_all_cameras()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = manager.get_frames()\n",
    "        for cam_id, frame in frames.items():\n",
    "            # print(cam_id, \"frame =\", type(frame))\n",
    "            if frame is not None:\n",
    "                cv2.imshow(cam_id, frame)\n",
    "\n",
    "        # small wait to allow events and reduce CPU usage\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # optional small sleep if you want lower CPU\n",
    "        # time.sleep(0.01)\n",
    "finally:\n",
    "    manager.stop_all_cameras()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5941fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added camera cam_1: Camera 1\n",
      "Created 1 cameras\n",
      "  cam_1: Camera 1 - rtsp://admin:admin%40123@192.168.0.110:554/stream1\n",
      "Started camera cam_1\n",
      "Started 1/1 cameras\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "Processing batch of 0 frames...\n",
      "All cameras stopped\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m             last_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# small wait to allow events and reduce CPU usage\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "from camera import CameraManager, create_camera_configs_from_ips  # Import the CameraManager class\n",
    "\n",
    "# Camera IPs and configurations\n",
    "camera_ips = [\n",
    "    \"192.168.0.110\"\n",
    "]\n",
    "\n",
    "# Create configurations\n",
    "camera_configs = create_camera_configs_from_ips(camera_ips)\n",
    "\n",
    "# Initialize camera manager\n",
    "manager = CameraManager(display_width=640, display_height=480)\n",
    "\n",
    "# Add cameras\n",
    "for i, config in enumerate(camera_configs):\n",
    "    camera_id = f\"cam_{i+1}\"\n",
    "    manager.add_camera(camera_id, config)\n",
    "\n",
    "print(f\"Created {manager.get_camera_count()} cameras\")\n",
    "for camera_id in manager.get_camera_ids():\n",
    "    camera = manager.get_camera(camera_id)\n",
    "    print(f\"  {camera_id}: {camera.get_name()} - {camera.get_config()['url']}\")\n",
    "\n",
    "# Define RTSP stream URL and output folder path\n",
    "output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'\n",
    "\n",
    "# Make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ⭐ START THE CAMERAS ⭐\n",
    "manager.start_all_cameras()\n",
    "\n",
    "try:\n",
    "    frame_batch = []  # To store frames for batch processing\n",
    "    batch_interval = 2  # 2 seconds interval to collect frames\n",
    "    last_time = time.time()  # Record the start time for batching\n",
    "\n",
    "    while True:\n",
    "        frames = manager.get_frames()\n",
    "        \n",
    "        # Collect frames for the current 2-second interval\n",
    "        for cam_id, frame in frames.items():\n",
    "            if frame is not None:\n",
    "                # Display frame (optional)\n",
    "                cv2.imshow(cam_id, frame)\n",
    "                \n",
    "                # Add the frame to the batch\n",
    "                frame_batch.append((cam_id, frame))\n",
    "        \n",
    "        # Check if 2 seconds have passed to process the batch\n",
    "        current_time = time.time()\n",
    "        if current_time - last_time >= batch_interval:\n",
    "            # Process the collected batch here\n",
    "            print(f\"Processing batch of {len(frame_batch)} frames...\")\n",
    "            \n",
    "            # Save frames in the specified folder\n",
    "            for cam_id, frame in frame_batch:\n",
    "                # Create a filename based on camera ID and current timestamp\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"{cam_id}_{timestamp}.jpg\"\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                \n",
    "                # Save the frame as an image\n",
    "                cv2.imwrite(file_path, frame)\n",
    "                print(f\"Saved {filename}\")\n",
    "            \n",
    "            # Reset for the next batch\n",
    "            frame_batch = []\n",
    "            last_time = current_time\n",
    "        \n",
    "        # small wait to allow events and reduce CPU usage\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    manager.stop_all_cameras()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42936518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added camera cam_1: Camera 1\n",
      "Created 1 cameras\n",
      "  cam_1: Camera 1 - rtsp://admin:admin%40123@192.168.0.110:554/stream1\n",
      "Started camera cam_1\n",
      "Started 1/1 cameras\n",
      "Camera cam_1 pipeline started at 1764849695003562732 ns (wall clock)\n",
      "Processing batch of 49 frames...\n",
      "All cameras stopped\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/smartan5070/Downloads/SlowfastTrainer-main/cam_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 206\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing batch of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(frame_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m frames...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Detect person in the first frame and apply YOLO cropping\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m best_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    208\u001b[0m max_area \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/engine/model.py:177\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    151\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    This method simplifies the process of making predictions by allowing the model instance to be called directly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/engine/model.py:535\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:225\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:298\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:255\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set up source and inference mode.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    source (str | Path | list[str] | list[Path] | list[np.ndarray] | np.ndarray | torch.Tensor): Source for\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        inference.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[1;32m    268\u001b[0m ):  \u001b[38;5;66;03m# long sequence\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/data/build.py:417\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, batch, vid_stride, buffer, channels)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_inference_source\u001b[39m(\n\u001b[1;32m    392\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    393\u001b[0m     batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     channels: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    397\u001b[0m ):\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load an inference source for object detection and apply necessary transformations.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m        >>> dataset = load_inference_source(\"rtsp://example.com/stream\", vid_stride=2)\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     source, stream, screenshot, from_img, in_memory, tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     source_type \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m SourceTypes(stream, screenshot, from_img, tensor)\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/data/build.py:379\u001b[0m, in \u001b[0;36mcheck_source\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    377\u001b[0m     in_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 379\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43mautocast_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# convert all list elements to PIL or np arrays\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     from_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (Image\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray)):\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/ultralytics/data/loaders.py:635\u001b[0m, in \u001b[0;36mautocast_list\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m source:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, (\u001b[38;5;28mstr\u001b[39m, Path)):  \u001b[38;5;66;03m# filename or uri\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, (Image\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray)):  \u001b[38;5;66;03m# PIL or np Image\u001b[39;00m\n\u001b[1;32m    637\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(im)\n",
      "File \u001b[0;32m~/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/smartan5070/Downloads/SlowfastTrainer-main/cam_1'"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import cv2\n",
    "# import os\n",
    "# import mlflow\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "# from camera import CameraManager, create_camera_configs_from_ips  # Import the CameraManager class\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "# from decord import VideoReader, cpu\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Clear GPU memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# def run_mvit_inference(video_path, run_id):\n",
    "#     \"\"\"\n",
    "#     Loads the MViT model from MLflow using the run_id and performs inference on the given video.\n",
    "#     \"\"\"\n",
    "#     # 1️⃣ Set experiment FIRST (assuming the experiment name is \"MViT_Testing\")\n",
    "#     mlflow.set_experiment(\"MViT_Testing\")\n",
    "\n",
    "#     # 2️⃣ Load the model from MLflow using the run ID\n",
    "#     model_uri = f\"runs:/{run_id}/best_model\"\n",
    "#     print(f\"Loading model from MLflow: {model_uri}\")\n",
    "\n",
    "#     # Load the model using the MLflow URI\n",
    "#     model_2 = mlflow.pytorch.load_model(model_uri)\n",
    "#     device = next(model_2.parameters()).device\n",
    "\n",
    "#     # Define video transformations\n",
    "#     transform = Compose([\n",
    "#         Resize((256, 256)),  # Resize input to 256x256\n",
    "#         CenterCrop(224),     # Crop to 224x224 after resizing\n",
    "#         NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])  # Normalize to match training conditions\n",
    "#     ])\n",
    "\n",
    "#     # Create a dataset and dataloader for the video\n",
    "#     frames_per_clip = 16  # Number of frames per clip (or adjust as needed)\n",
    "#     test_dataset = FlatVideoDataset(video_path, transform=transform, frames_per_clip=16)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\n",
    "\n",
    "#     # Perform inference\n",
    "#     video_predictions = run_inference(test_loader, model_2, device)\n",
    "\n",
    "#     # -------------------- NEW: AUTO-LOAD TRAINING CLASS NAMES --------------------\n",
    "#     train_root = \"/home/smartan5070/Downloads/SlowfastTrainer-main/Dataset_30Classes_Cam107-18_SPLIT/train\"\n",
    "\n",
    "#     # Auto-read folder names in alphabetical order — EXACT training class order\n",
    "#     class_names = sorted(os.listdir(train_root))\n",
    "\n",
    "#     # Map index → class name automatically\n",
    "#     idx_to_class_name = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "#     for path, prediction_idx in video_predictions.items():\n",
    "#         predicted_class = idx_to_class_name.get(prediction_idx, f\"UNKNOWN_INDEX_{prediction_idx}\")\n",
    "#         print(f\"File: {os.path.basename(path):<50} -> Predicted Class: {predicted_class}\")\n",
    "\n",
    "# # ============================\n",
    "# # Dataset and Inference\n",
    "# # ============================\n",
    "\n",
    "# class FlatVideoDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root_dir, transform=None, frames_per_clip=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.frames_per_clip = frames_per_clip\n",
    "#         self.video_paths = []\n",
    "#         self._build_index()\n",
    "\n",
    "#     def _build_index(self):\n",
    "#         if os.path.isfile(self.root_dir) and self.root_dir.lower().endswith(\".mp4\"):\n",
    "#             # Single video file\n",
    "#             self.video_paths.append(self.root_dir)\n",
    "#         elif os.path.isdir(self.root_dir):\n",
    "#             # Directory of videos\n",
    "#             for fname in os.listdir(self.root_dir):\n",
    "#                 if fname.lower().endswith(\".mp4\"):\n",
    "#                     self.video_paths.append(os.path.join(self.root_dir, fname))\n",
    "#         else:\n",
    "#             raise ValueError(f\"{self.root_dir} is neither a .mp4 file nor a directory\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.video_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         path = self.video_paths[idx]\n",
    "#         label = -1  # Dummy label\n",
    "        \n",
    "#         try:\n",
    "#             vr = VideoReader(path, ctx=cpu(0))\n",
    "#             total_frames = len(vr)\n",
    "\n",
    "#             if total_frames < self.frames_per_clip:\n",
    "#                 base = np.linspace(0, total_frames - 1, total_frames).astype(int)\n",
    "#                 pad = self.frames_per_clip - total_frames\n",
    "#                 frame_indices = np.concatenate([base, np.full((pad,), base[-1], dtype=int)])\n",
    "#             else:\n",
    "#                 frame_indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "\n",
    "#             frames = vr.get_batch(frame_indices).asnumpy()  # (T,H,W,C)\n",
    "\n",
    "#             if frames.shape[-1] == 1:\n",
    "#                 frames = np.repeat(frames, 3, axis=-1)\n",
    "#             elif frames.shape[-1] != 3:\n",
    "#                 raise ValueError(f\"Unsupported channel count: {frames.shape[-1]} in video {path}\")\n",
    "\n",
    "#             frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0   \n",
    "\n",
    "#             if self.transform:\n",
    "#                 frames = self.transform(frames)\n",
    "\n",
    "#             return frames, label, path \n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load video: {path}\\nError: {e}\")\n",
    "#             return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "# # Assuming NormalizeVideo class is defined or is part of torchvision.transforms\n",
    "# class NormalizeVideo(nn.Module):\n",
    "#     def __init__(self, mean, std):\n",
    "#         super().__init__()\n",
    "#         self.mean = torch.tensor(mean).view(3, 1, 1, 1)\n",
    "#         self.std = torch.tensor(std).view(3, 1, 1, 1)\n",
    "#     def forward(self, tensor):\n",
    "#         return (tensor - self.mean) / self.std\n",
    "\n",
    "# def run_inference(test_loader, model, device):\n",
    "#     all_predictions = []\n",
    "#     all_video_paths = []\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _, paths in tqdm(test_loader, desc=\"Inference\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             print(f\"Predicted: {predicted}\")\n",
    "#             all_predictions.extend(predicted.cpu().numpy().tolist())\n",
    "#             all_video_paths.extend(paths)\n",
    "\n",
    "#     return dict(zip(all_video_paths, all_predictions))\n",
    "\n",
    "\n",
    "# # Camera IPs and configurations\n",
    "# camera_ips = [\n",
    "#     \"192.168.0.110\"\n",
    "# ]\n",
    "\n",
    "# # Create configurations\n",
    "# camera_configs = create_camera_configs_from_ips(camera_ips)\n",
    "\n",
    "# # Initialize camera manager\n",
    "# manager = CameraManager(display_width=640, display_height=480)\n",
    "\n",
    "# # Add cameras\n",
    "# for i, config in enumerate(camera_configs):\n",
    "#     camera_id = f\"cam_{i+1}\"\n",
    "#     manager.add_camera(camera_id, config)\n",
    "\n",
    "# print(f\"Created {manager.get_camera_count()} cameras\")\n",
    "# for camera_id in manager.get_camera_ids():\n",
    "#     camera = manager.get_camera(camera_id)\n",
    "#     print(f\"  {camera_id}: {camera.get_name()} - {camera.get_config()['url']}\")\n",
    "\n",
    "# # Define output folder path\n",
    "# output_folder = '/home/smartan5070/Downloads/SlowfastTrainer-main/unseen_test/cropped_frames'\n",
    "\n",
    "# # Make sure the output folder exists\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # ⭐ START THE CAMERAS ⭐\n",
    "# manager.start_all_cameras()\n",
    "\n",
    "# try:\n",
    "#     frame_batch = []  # To store frames for batch processing\n",
    "#     batch_interval = 2  # 2 seconds interval to collect frames\n",
    "#     last_time = time.time()  # Record the start time for batching\n",
    "#     frame_count = 0\n",
    "\n",
    "#     model_yolo = YOLO(\"yolov8n.pt\")  # Load YOLOv8 model for person detection\n",
    "\n",
    "#     while True:\n",
    "#         frames = manager.get_frames()\n",
    "        \n",
    "#         # Collect frames for the current 2-second interval\n",
    "#         for cam_id, frame in frames.items():\n",
    "#             if frame is not None:\n",
    "#                 # Display frame (optional)\n",
    "#                 cv2.imshow(cam_id, frame)\n",
    "                \n",
    "#                 # Add the frame to the batch\n",
    "#                 frame_batch.append((cam_id, frame))\n",
    "#                 # frame_count += 1\n",
    "        \n",
    "#         # Check if 2 seconds have passed to process the batch\n",
    "#         current_time = time.time()\n",
    "#         if current_time - last_time >= batch_interval:\n",
    "#             if frame_batch:\n",
    "#                 print(f\"Processing batch of {len(frame_batch)} frames...\")\n",
    "\n",
    "#                 # Detect person in the first frame and apply YOLO cropping\n",
    "#                 results = model_yolo(frame_batch[0][1], conf=0.5, verbose=False)\n",
    "#                 best_box = None\n",
    "#                 max_area = 0\n",
    "\n",
    "#                 # Find the largest bounding box for the person detected\n",
    "#                 for r in results:\n",
    "#                     for box in r.boxes:\n",
    "#                         if box.cls.item() == 0:  # Person class ID in YOLO\n",
    "#                             x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # Get the bounding box coordinates\n",
    "\n",
    "#                             # Clamp the bounding box to ensure it stays within the frame boundaries\n",
    "#                             H, W = frame.shape[:2]  # Get the height and width of the frame\n",
    "#                             clamped = clamp_bbox((x1, y1, x2, y2), H, W)  # Clamping function to stay within frame limits\n",
    "                            \n",
    "#                             if not clamped:\n",
    "#                                 continue  # Skip if clamping failed (invalid box)\n",
    "\n",
    "#                             x1, y1, x2, y2 = clamped  # Unpack the clamped bounding box coordinates\n",
    "\n",
    "#                             # Now, crop the frame using the bounding box\n",
    "#                             crop_bgr = frame[y1:y2, x1:x2]  # Crop using the bounding box\n",
    "#                             if crop_bgr is None or crop_bgr.size == 0:\n",
    "#                                 continue  # Skip if the crop is empty\n",
    "\n",
    "#                             # Continue processing the cropped image (e.g., sending it to SlowFast)\n",
    "#                             print(f\"Cropped frame size: {crop_bgr.shape}\")\n",
    "\n",
    "#                 # If a person is detected, crop the frames\n",
    "#                 if best_box is not None:\n",
    "#                     # Get height and width of frames\n",
    "#                     frame_height, frame_width, _ = frame_batch[0][1].shape\n",
    "#                     x1, y1, x2, y2 = best_box\n",
    "                    \n",
    "#                     # Apply padding to the bounding box\n",
    "#                     PADDING = 15\n",
    "#                     padded_box = (\n",
    "#                         max(0, x1 - PADDING),\n",
    "#                         max(0, y1 - PADDING),\n",
    "#                         min(frame_width, x2 + PADDING),\n",
    "#                         min(frame_height, y2 + PADDING)\n",
    "#                     )\n",
    "\n",
    "#                     # Crop the frames based on the detected bounding box\n",
    "#                     cropped_batch = []\n",
    "#                     for cam_id, frame in frame_batch:\n",
    "#                         cropped_frame = frame[padded_box[1]:padded_box[3], padded_box[0]:padded_box[2]]\n",
    "#                         cropped_batch.append(cropped_frame)\n",
    "\n",
    "#                     # Save the cropped frames to a new video\n",
    "#                     timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#                     video_filename = f\"cropped_video_{timestamp}_{frame_count}.mp4\"\n",
    "#                     video_path = os.path.join(output_folder, video_filename)\n",
    "\n",
    "#                     # Initialize VideoWriter to save the cropped video\n",
    "#                     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#                     video_writer = cv2.VideoWriter(video_path, fourcc, 30, (cropped_batch[0].shape[1], cropped_batch[0].shape[0]))\n",
    "\n",
    "#                     # Write the cropped frames to the video file\n",
    "#                     for cropped_frame in cropped_batch:\n",
    "#                         video_writer.write(cropped_frame)\n",
    "\n",
    "#                     # Release the VideoWriter after saving the video\n",
    "#                     video_writer.release()\n",
    "#                     print(f\"Saved cropped video: {video_filename}\")\n",
    "\n",
    "#                     # Run inference on the saved cropped video\n",
    "#                     run_id = \"840f3d39813c41ba9880859c83a82b01\"  # Replace with the actual run ID\n",
    "#                     run_mvit_inference(video_path, run_id)  # Run inference on the cropped video\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"No person detected, skipping cropping and video saving.\")\n",
    "\n",
    "#             # Reset for the next batch\n",
    "#             frame_batch = []\n",
    "#             last_time = current_time\n",
    "\n",
    "#         # Small wait to allow events and reduce CPU usage\n",
    "#         if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "# finally:\n",
    "#     manager.stop_all_cameras()\n",
    "#     cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25e463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact from MLflow run: e14ed2fa6acb4bfea77caba82fca0652\n",
      "/home/smartan5070/Downloads/SlowfastTrainer-main/virenv/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "Downloading artifacts:   0%|                              | 0/1 [00:00<?, ?it/s]\n",
      "Downloading artifacts: 100%|██████████████████████| 6/6 [00:00<00:00, 35.90it/s]\n",
      "Saved model state_dict to: /home/smartan5070/Downloads/SlowfastTrainer-main/downloaded_models/model_4.pt\n"
     ]
    }
   ],
   "source": [
    "!python mlflow2ptmodel.py --run_id \"e14ed2fa6acb4bfea77caba82fca0652\" --model_name \"model_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a1e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
