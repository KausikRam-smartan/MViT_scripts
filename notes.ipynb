{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef4a98e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Comprehensive Guide to Video-Specific Augmentations for Action Recognition**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In action recognition tasks, **augmentation** is a powerful tool for improving generalization and preventing overfitting. By artificially modifying the data during training, augmentations force the model to focus on meaningful patterns‚Äîsuch as **motion**‚Äîrather than memorizing specific details like exact timings, locations, or backgrounds. This approach is crucial for learning robust features and improving the model‚Äôs ability to generalize to unseen data.\n",
    "\n",
    "This guide provides a detailed overview of both **temporal** and **spatial** augmentations, along with some strong regularization techniques and practices to avoid.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Temporal Augmentations**\n",
    "\n",
    "### üü¶ **Random Clip Start**\n",
    "\n",
    "* **What It Means**: Rather than always using a fixed portion of a video (e.g., frames [0...15]), the clip start is randomized within the video.\n",
    "\n",
    "  * Example:\n",
    "\n",
    "    * If the video has 120 frames and you need a clip of 16 frames:\n",
    "\n",
    "      * **Epoch 1**: Frames 10‚Äì25\n",
    "      * **Epoch 2**: Frames 42‚Äì57\n",
    "      * **Epoch 3**: Frames 80‚Äì95\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * Forces the model to learn **action** patterns without memorizing specific time intervals.\n",
    "  * Allows the model to see different sections of the video, increasing diversity in training and forcing the model to focus on **motion** rather than on exact timings.\n",
    "  * **Benefit**: Helps the model to generalize better to different temporal positions in the video.\n",
    "\n",
    "### üü¶ **Random Frame Stride**\n",
    "\n",
    "* **What It Means**: Instead of using every frame, you randomly skip frames by setting a **frame stride**.\n",
    "\n",
    "  * Example:\n",
    "\n",
    "    * **Stride = 1**: Use every frame.\n",
    "    * **Stride = 2**: Use every second frame.\n",
    "    * **Stride = 3**: Use every third frame.\n",
    "  * The stride should be randomized per sample, meaning that different strides can be applied across the same dataset.\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * Encourages **temporal robustness**, allowing the model to recognize actions even when the frame rate or speed of motion varies.\n",
    "  * Prevents the model from memorizing **specific motion patterns** and promotes better **generalization**.\n",
    "  * **Benefit**: The model becomes more robust to variations in video frame rates and motion speed.\n",
    "\n",
    "### üü¶ **Temporal Jitter (Drop / Duplicate Frames)**\n",
    "\n",
    "* **What It Means**: Introduces slight perturbations in the temporal order by either randomly **dropping a frame** or **duplicating a frame** in a sequence.\n",
    "\n",
    "  * Example:\n",
    "\n",
    "    * Drop a frame at random positions in the sequence.\n",
    "    * Duplicate a frame to simulate jitter.\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * Simulates **irregularities** in video capture (e.g., camera jitter, frame drops).\n",
    "  * Forces the model to **ignore minor inconsistencies** in time and still recognize the action.\n",
    "  * **Benefit**: Enhances the model‚Äôs robustness to noisy or imperfect data.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Spatial Augmentations**\n",
    "\n",
    "Spatial augmentations manipulate the **appearance** of frames to prevent the model from memorizing visual details and focus more on **motion** or **action patterns**.\n",
    "\n",
    "### ‚úÖ **Recommended Spatial Augmentation (Training)**\n",
    "\n",
    "#### **Transform Pipeline**:\n",
    "\n",
    "```python\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    RandomResizedCropVideo,\n",
    "    RandomHorizontalFlipVideo,\n",
    "    NormalizeVideo\n",
    ")\n",
    "\n",
    "transform = Compose([\n",
    "    RandomResizedCropVideo(\n",
    "        size=(224, 224),\n",
    "        scale=(0.8, 1.0),  # Mild zoom\n",
    "        ratio=(3/4, 4/3)    # Random aspect ratio\n",
    "    ),\n",
    "    RandomHorizontalFlipVideo(p=0.5),  # Flip 50% of the time\n",
    "    NormalizeVideo([0.45, 0.45, 0.45],  # Mean values for normalization\n",
    "                   [0.225, 0.225, 0.225])  # Standard deviation values\n",
    "])\n",
    "```\n",
    "\n",
    "* **Why It Works**:\n",
    "\n",
    "  * **RandomResizedCropVideo**: Randomly crops and resizes each frame, preventing the model from memorizing specific regions of the frame and making the model more robust to different viewpoints and perspectives.\n",
    "  * **RandomHorizontalFlipVideo**: Applies a horizontal flip to 50% of the frames, helping the model handle **left-right symmetry** and improving generalization.\n",
    "  * **NormalizeVideo**: Standardizes pixel values to help the model converge faster and be less sensitive to brightness and contrast variations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Fallback When `RandomResizedCropVideo` is Unavailable**:\n",
    "\n",
    "If `RandomResizedCropVideo` is not available in your version of `torchvision`, use the following alternative:\n",
    "\n",
    "```python\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms._transforms_video import NormalizeVideo\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip\n",
    "\n",
    "transform = Compose([\n",
    "    RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    NormalizeVideo([0.45, 0.45, 0.45],  # Normalize\n",
    "                   [0.225, 0.225, 0.225])\n",
    "])\n",
    "```\n",
    "\n",
    "* **Why It Works**:\n",
    "\n",
    "  * Even though `RandomResizedCrop` operates on individual frames (not videos), since the video tensor is of shape `(C, T, H, W)`, this operation will be applied to each frame independently.\n",
    "  * The normalization ensures that the model learns robust features independent of color or lighting conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **Validation vs. Training Transforms**\n",
    "\n",
    "It‚Äôs crucial to **separate** the transforms used for **training** and **validation**.\n",
    "\n",
    "* **Training Transforms**: Randomized to encourage generalization (e.g., random crop, flip, jitter).\n",
    "* **Validation Transforms**: Fixed transformations to ensure stable and consistent evaluation.\n",
    "\n",
    "#### Example for Validation/Testing Transforms:\n",
    "\n",
    "```python\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms._transforms_video import ResizeVideo, NormalizeVideo\n",
    "\n",
    "val_transform = Compose([\n",
    "    ResizeVideo((224, 224)),  # Resizing to fixed size for stable evaluation\n",
    "    NormalizeVideo([0.45, 0.45, 0.45],  # Mean normalization\n",
    "                   [0.225, 0.225, 0.225])  # Standard deviation normalization\n",
    "])\n",
    "```\n",
    "\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * **Stable evaluation**: By resizing and centering the frames during validation, we ensure that the model is not confused by random augmentations and can make an accurate, consistent evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Why Temporal + Spatial Augmentations Work Together**\n",
    "\n",
    "When combining **temporal** and **spatial augmentations**, the model learns to generalize across **both time** and **space**, leading to better overall performance.\n",
    "\n",
    "### **Benefits**:\n",
    "\n",
    "* **Temporal Augmentations**: Prevent the model from memorizing **specific action timing**.\n",
    "* **Spatial Augmentations**: Prevent the model from memorizing the **appearance** of the action.\n",
    "\n",
    "### **Expected Outcome**:\n",
    "\n",
    "* **Train Accuracy**: Slightly lower (expected due to increased regularization).\n",
    "* **Validation Accuracy**: Slightly lower.\n",
    "* **Test Accuracy**: Significantly higher.\n",
    "* **Overfitting Gap**: Decreases as the model generalizes better.\n",
    "\n",
    "Together, these augmentations force the model to focus on recognizing **motion** and **action** across different **temporal** and **spatial** contexts, rather than memorizing specific video frames or segments.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Strong Regularization Methods**\n",
    "\n",
    "### **Effective Techniques for Reducing Overfitting**:\n",
    "\n",
    "#### **Label Smoothing (0.05‚Äì0.1)**:\n",
    "\n",
    "* **What It Means**: Instead of assigning a hard probability (e.g., 1.0 for the correct class), you smooth the labels by slightly reducing the confidence in the correct class.\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * **Reduces overconfidence**: It prevents the model from becoming overly confident about specific predictions, which can lead to overfitting.\n",
    "  * Encourages the model to be more uncertain and learn more generalizable features.\n",
    "\n",
    "#### **MixUp (Video-Level)**:\n",
    "\n",
    "* **What It Means**: Mix two video clips by blending their pixel values and labels in a certain ratio.\n",
    "* **Why It Helps**:\n",
    "\n",
    "  * **Regularizes the model** by forcing it to learn from interpolated examples.\n",
    "  * Makes the model robust to variations in data and helps it learn more generalizable features.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Augmentations to Avoid**\n",
    "\n",
    "While many augmentations are beneficial, **certain ones** can disrupt the integrity of action recognition tasks.\n",
    "\n",
    "### **Avoid These Augmentations**:\n",
    "\n",
    "* **Random Rotation**: Alters the action semantics (e.g., rotating a person‚Äôs body).\n",
    "* **Perspective Transformations**: Creates unrealistic, distorted motion patterns.\n",
    "* **Elastic Transformations**: Distorts motion too heavily, breaking action structure.\n",
    "* **Heavy Blur**: Kills motion-related information, which is critical for action recognition.\n",
    "* **CutMix (Frame-Level)**: Breaks temporal consistency by cutting and mixing frames from different videos.\n",
    "* **Random Erasing**: Can remove key parts of the action (e.g., the person‚Äôs body or face), making it difficult for the model to recognize the action.\n",
    "* **Strong Cropping**: Can crop out critical parts of the body or action, which reduces the model‚Äôs\n",
    "\n",
    "\n",
    "ability to recognize the complete action.\n",
    "\n",
    "These augmentations can distort the natural motion or break the temporal and spatial continuity of the action, ultimately reducing the model's ability to recognize actions accurately.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Conclusion**\n",
    "\n",
    "By carefully applying **temporal** and **spatial augmentations**, along with **regularization** techniques, you can significantly enhance the model's robustness and ability to generalize. The goal is to force the model to focus on learning **general motion patterns** and **actions** instead of memorizing specific video frames, leading to improved performance on unseen data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d38fa8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2125cdaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Action Recognition Models: Overview and Recommendations**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In the realm of **action recognition**, transformer-based models are becoming increasingly popular due to their ability to capture **spatial** and **temporal** dependencies. Several models are available, each with unique strengths, weaknesses, and specific use cases. Below is an overview of the most relevant models, from **Video Swin Transformer** to **VideoMAE**, with recommendations on when to use each.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Video Swin Transformer (Recommended)**\n",
    "\n",
    "### **Why it‚Äôs Good**:\n",
    "\n",
    "* **Hierarchical Architecture**:\n",
    "\n",
    "  * More stable and efficient compared to ViT-style (Vision Transformer).\n",
    "  * It captures both **local and global temporal dependencies** effectively, making it ideal for video tasks.\n",
    "\n",
    "* **Pretrained on Large Datasets**:\n",
    "\n",
    "  * Trained on **Kinetics-400/600**, providing strong pretrained weights out-of-the-box.\n",
    "\n",
    "* **Supports 32-frame Clips**:\n",
    "\n",
    "  * Optimized for handling **32-frame input** clips, making it suitable for common action recognition tasks.\n",
    "\n",
    "* **Strong Performance & Efficiency**:\n",
    "\n",
    "  * Excellent at balancing **performance** and **computational efficiency**.\n",
    "\n",
    "### **Common Configurations**:\n",
    "\n",
    "* **Swin-T**: 32 √ó 224 √ó 224\n",
    "* **Swin-B**: 32 √ó 224 √ó 224\n",
    "\n",
    "### **Ideal For**:\n",
    "\n",
    "* **General-purpose action recognition**.\n",
    "* You need **32-frame clips** and want **strong pretrained weights** with **easy fine-tuning**.\n",
    "* You want an efficient and stable model that works well in real-world applications.\n",
    "\n",
    "### **Where to Find**:\n",
    "\n",
    "* **mmAction2**\n",
    "* **HuggingFace**\n",
    "* **Official Microsoft Swin Repo**\n",
    "\n",
    "### **Use this if**:\n",
    "\n",
    "üëâ You want **32 frames**, strong **pretrained weights**, and easy **fine-tuning**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. TimeSformer (ViT-Based)**\n",
    "\n",
    "### **Key Details**:\n",
    "\n",
    "* **Pure Transformer**: Unlike Video Swin, TimeSformer operates as a **pure transformer** over **space and time**.\n",
    "\n",
    "* **Pretrained Models Available**: Supports **32-frame clips** and offers pretrained variants:\n",
    "\n",
    "  * **TimeSformer-8**: 8 frames\n",
    "  * **TimeSformer-16**: 16 frames\n",
    "  * **TimeSformer-32**: 32 frames\n",
    "\n",
    "* **Memory and Data-Hungry**:\n",
    "\n",
    "  * Requires more memory and **larger datasets** compared to Swin.\n",
    "  * **High computational demand** for training.\n",
    "\n",
    "### **Tradeoffs**:\n",
    "\n",
    "* **Higher Memory Usage**: Compared to Swin, TimeSformer has a **higher memory footprint**.\n",
    "* **Data-Hungry**: Requires **strong augmentation** and **large datasets** to achieve optimal results.\n",
    "\n",
    "### **Ideal For**:\n",
    "\n",
    "* **ViT-style temporal transformer** models.\n",
    "* Use if you prefer a **pure transformer-based model** for video understanding, and are prepared to handle the **memory demands** and **data requirements**.\n",
    "\n",
    "### **Use this if**:\n",
    "\n",
    "üëâ You want a **true ViT-style temporal transformer** with advanced **space-time attention** capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. VideoMAE (VERY Strong)**\n",
    "\n",
    "### **Why it‚Äôs Strong**:\n",
    "\n",
    "* **Self-Supervised Pretraining**:\n",
    "\n",
    "  * **VideoMAE** excels due to its **self-supervised pretraining**, making it a powerful choice when **labeled data** is scarce.\n",
    "\n",
    "* **Excellent Temporal Modeling**:\n",
    "\n",
    "  * Strong at capturing **temporal dynamics** and long-range dependencies across frames.\n",
    "\n",
    "* **Works Well with Limited Data**:\n",
    "\n",
    "  * **Self-supervised pretraining** allows VideoMAE to perform well even with limited labeled data.\n",
    "\n",
    "### **Models**:\n",
    "\n",
    "* **videomae-base-32**: 32-frame model, smaller configuration.\n",
    "* **videomae-large-32**: 32-frame model, larger configuration for more complex tasks.\n",
    "\n",
    "### **Ideal For**:\n",
    "\n",
    "* You need **state-of-the-art performance** and can **fine-tune** the model on a specific dataset.\n",
    "* You want **superior temporal modeling** capabilities and can afford the **heavier training pipeline**.\n",
    "\n",
    "### **Use this if**:\n",
    "\n",
    "üëâ You want **best performance** with **self-supervised pretraining** and are fine-tuning on your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. MViT (Modified, Not Recommended Initially)**\n",
    "\n",
    "### **Status**:\n",
    "\n",
    "* **Still Relevant**: **MViT** is still considered a **relevant and efficient model**, but it is not the top choice for all tasks anymore, especially when compared to newer models like **Video Swin** or **VideoMAE**.\n",
    "\n",
    "### **Why it Matters**:\n",
    "\n",
    "* **Engineered for Video**:\n",
    "\n",
    "  * Specifically designed for video tasks with multi-scale temporal resolution.\n",
    "  * Efficient in terms of **compute** and capable of handling multiple video scales.\n",
    "\n",
    "* **Pretrained on 16 Frames**:\n",
    "\n",
    "  * Most pretrained configurations are **limited to 16 frames**, making it less flexible for working with longer clip lengths.\n",
    "\n",
    "### **Tradeoffs**:\n",
    "\n",
    "* **Less Flexible**:\n",
    "\n",
    "  * Struggles with longer video clips beyond 16 frames.\n",
    "  * **Interpolate temporal positional embeddings** and change temporal stride if needed, but performance may degrade.\n",
    "\n",
    "### **Ideal For**:\n",
    "\n",
    "* Real-world systems requiring **efficient video understanding**.\n",
    "* You need a **compute-efficient solution** that works well for **multi-scale video data**.\n",
    "\n",
    "### **Use this if**:\n",
    "\n",
    "üëâ You need **compute-efficient models** for video and are dealing with **16-frame clips**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table: Model Overview**\n",
    "\n",
    "| **Model**       | **Category**                | **SOTA Status**    | **Key Idea**                               |\n",
    "| --------------- | --------------------------- | ------------------ | ------------------------------------------ |\n",
    "| **MViT**        | Multiscale Transformer      | ‚úÖ SOTA (Efficient) | Multi-scale temporal + spatial attention   |\n",
    "| **TimeSformer** | Pure ViT                    | ‚ö†Ô∏è Early SOTA      | Factorized space‚Äìtime attention            |\n",
    "| **Video Swin**  | Hierarchical Transformer    | ‚úÖ Strong SOTA      | Local window attention + temporal modeling |\n",
    "| **VideoMAE**    | Self-supervised Pretraining | ‚úÖ Current SOTA     | Masked video modeling                      |\n",
    "\n",
    "---\n",
    "\n",
    "## **Detailed Model Breakdown**\n",
    "\n",
    "### **MViT**\n",
    "\n",
    "* **Status**: ‚úÖ SOTA-relevant, especially for **engineering-focused** tasks.\n",
    "* **Why it‚Äôs Important**:\n",
    "\n",
    "  * **Efficient** and designed specifically for **video processing**.\n",
    "  * Widely used in **real-world applications**, particularly at **Meta**.\n",
    "* **Limitation**:\n",
    "\n",
    "  * Most pretrained models are **limited to 16 frames**. Less flexible for temporal length adjustments.\n",
    "* **When to Use**:\n",
    "\n",
    "  * When you need **efficiency** and are working with **16-frame videos**.\n",
    "\n",
    "### **TimeSformer**\n",
    "\n",
    "* **Status**: ‚ö†Ô∏è Early SOTA, but no longer top-tier.\n",
    "* **Why it was Important**:\n",
    "\n",
    "  * **First ViT model** that demonstrated effective video recognition.\n",
    "  * Clean and effective **space-time attention factorization**.\n",
    "* **Weaknesses**:\n",
    "\n",
    "  * **High memory usage** and **data-hungry**.\n",
    "  * No hierarchy, which can limit performance on large-scale datasets.\n",
    "* **When to Use**:\n",
    "\n",
    "  * Use for **conceptual importance** or when working with smaller datasets but beware of its limitations.\n",
    "\n",
    "### **Video Swin Transformer**\n",
    "\n",
    "* **Status**: ‚úÖ Strong modern SOTA.\n",
    "* **Why it‚Äôs Popular**:\n",
    "\n",
    "  * **Hierarchical architecture**, similar to CNNs.\n",
    "  * **Window-based attention** allows for scalability.\n",
    "* **When to Use**:\n",
    "\n",
    "  * When you need **32-frame clips**, strong **pretrained weights**, and efficient **fine-tuning**.\n",
    "\n",
    "### **VideoMAE**\n",
    "\n",
    "* **Status**: ‚úÖ Current SOTA leader.\n",
    "* **Why it Dominates**:\n",
    "\n",
    "  * **Self-supervised pretraining** excels with limited labeled data.\n",
    "  * Superior **temporal modeling** compared to other models.\n",
    "* **When to Use**:\n",
    "\n",
    "  * When you want **best performance** and are fine-tuning on your custom dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "### **Best Model for You?**\n",
    "\n",
    "* **For Stability and Ease of Fine-Tuning**: Go with **Video Swin Transformer**.\n",
    "* **For True Transformer Architecture**: Choose **TimeSformer** if you're focused on **space-time factorization**.\n",
    "* **For Best Performance**: If you can fine-tune and need cutting-edge results, **VideoMAE** is your best bet.\n",
    "* **For Compute Efficiency**: Use **MViT** if you prioritize **efficient, multi-scale video processing**.\n",
    "\n",
    "Each of these models has its unique strengths, and your choice should depend on your specific requirements, such as data availability, compute resources, and the length of video clips you intend to process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc35c65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b2ef05",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Label smoothing (very effective, very safe)\n",
    "\n",
    "This reduces overconfidence, which is a big issue for transformers.\n",
    "\n",
    "Effect\n",
    "\n",
    "Lowers train accuracy slightly\n",
    "\n",
    "Improves test accuracy\n",
    "\n",
    "Stabilizes logits\n",
    "\n",
    "Typical value\n",
    "\n",
    "0.1\n",
    "\n",
    "This is one of the best low-risk additions.\n",
    "\n",
    "\n",
    "9Ô∏è‚É£ Smaller head (optional)\n",
    "\n",
    "If your head is large:\n",
    "\n",
    "Reduce hidden dimensions\n",
    "\n",
    "Keep it shallow\n",
    "\n",
    "Big heads overfit fast.\n",
    "\n",
    "\n",
    "Summary of Recommendations\n",
    "\n",
    "Regularization:\n",
    "\n",
    "Apply label smoothing (0.05‚Äì0.1).\n",
    "\n",
    "Increase weight decay to 0.1‚Äì0.2.\n",
    "\n",
    "Use early stopping during training.\n",
    "\n",
    "Augmentation:\n",
    "\n",
    "Use temporal jittering and more aggressive spatial augmentation (e.g., random resized crop, color jitter, random grayscale).\n",
    "\n",
    "Experiment with MixUp or CutMix.\n",
    "\n",
    "Model Adjustments:\n",
    "\n",
    "Try reducing model complexity or reducing the number of layers in MViT.\n",
    "\n",
    "Consider using global average pooling to reduce overfitting.\n",
    "\n",
    "Learning Rate:\n",
    "\n",
    "Use a lower learning rate and implement a learning rate scheduler (e.g., StepLR or CosineAnnealing).\n",
    "\n",
    "Training Split:\n",
    "\n",
    "Use the 70-30 split or stratified sampling for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30109568",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2316cd",
   "metadata": {},
   "source": [
    "1. Tuning Weight Decay (weight_decay)\n",
    "\n",
    "The weight decay parameter in AdamW is a form of L2 regularization that prevents the model from overfitting by penalizing large weights. Too much weight decay can overly penalize the model's weights, making it less flexible and leading to poor test performance. On the other hand, too little weight decay might not prevent overfitting enough, especially if your model has a high capacity.\n",
    "\n",
    "Here are some suggestions:\n",
    "\n",
    "Try smaller increments: Instead of jumping directly from 0.05 to 0.1, try values like:\n",
    "\n",
    "0.01, 0.05, 0.075.\n",
    "\n",
    "0.03, 0.07.\n",
    "\n",
    "Check if smaller values work better, as they might retain more flexibility in the model without causing overfitting.\n",
    "\n",
    "Lower values of weight decay:\n",
    "\n",
    "Since a high weight decay of 0.1 caused performance drops, try reducing it slightly. For example, try 0.01 and 0.05.\n",
    "\n",
    "Example:\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "\n",
    "Gradual Warm-up: Combine weight decay with a learning rate warm-up to avoid destabilizing training at the beginning. Gradually increase the learning rate in the first few epochs.\n",
    "\n",
    "2. Tuning Label Smoothing (label_smoothing)\n",
    "\n",
    "Label smoothing is a technique that helps reduce overfitting by making the model less confident in its predictions. A higher value for label smoothing (e.g., 0.1) could reduce overfitting further by spreading the probability across other classes, but it can also hurt the model's performance if set too high.\n",
    "\n",
    "How to fine-tune label smoothing:\n",
    "\n",
    "0.05: A mild label smoothing value, as you have now, generally works well for most models.\n",
    "\n",
    "Increase slowly: If you feel the model needs more regularization, try increasing label smoothing to 0.1 or 0.2, but go in small increments.\n",
    "\n",
    "Example:\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "\n",
    "Test with no label smoothing: If label smoothing at 0.05 reduced performance, try training without label smoothing first and monitor the test set accuracy. It could help you determine if it‚Äôs actually helping or hurting performance.\n",
    "\n",
    "3. Learning Rate Tuning\n",
    "\n",
    "The learning rate is also crucial. If it's too high, the model might not converge properly. If it's too low, it could lead to slow convergence or getting stuck in suboptimal minima. You may want to experiment with slightly smaller or larger values.\n",
    "\n",
    "Learning Rate Adjustments:\n",
    "\n",
    "Reduce learning rate: If your model is not generalizing well, lower the learning rate slightly. For example, reduce the learning rate from lr = 0.001 to lr = 0.0005 or lr = 0.0001.\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "\n",
    "\n",
    "Learning Rate Scheduling: To prevent overshooting the optimal solution, use a learning rate scheduler like CosineAnnealingLR or StepLR to adjust the learning rate during training dynamically.\n",
    "\n",
    "4. Other Regularization Techniques\n",
    "A. Dropout in Model\n",
    "\n",
    "Since you're already applying 0.5 dropout on the classification head, consider:\n",
    "\n",
    "Testing dropout at different rates (e.g., 0.3, 0.4, 0.6). Too much dropout can make training unstable or overly aggressive, and too little may not provide enough regularization.\n",
    "\n",
    "Try applying dropout to other parts of the model like hidden layers, especially if overfitting is happening in the earlier stages of training.\n",
    "\n",
    "B. Early Stopping\n",
    "\n",
    "Early stopping prevents your model from overfitting by halting training when the validation loss stops improving for a set number of epochs. If the model‚Äôs accuracy on the test set is dropping after 10 epochs, early stopping could prevent overfitting from becoming too severe.\n",
    "\n",
    "C. More Temporal Augmentations (Hybrid Augmentation)\n",
    "\n",
    "You mentioned you‚Äôre applying temporal augmentation, but it's possible that too much regularization is affecting the model‚Äôs ability to learn. Try mixing spatial and temporal augmentations with a lower probability (e.g., reduce the flipping probability to 0.25).\n",
    "\n",
    "Try milder temporal augmentations to avoid drastic changes in the temporal patterns, such as:\n",
    "\n",
    "Random frame stride with low strides (e.g., 2 or 3 frames)\n",
    "\n",
    "Random frame drop (e.g., drop 1‚Äì2 frames randomly)\n",
    "\n",
    "Mild temporal jitter: Try adding a slight temporal jitter (e.g., drop/duplicate one frame every few frames).\n",
    "\n",
    "Recommended Next Steps:\n",
    "\n",
    "Optimize Weight Decay:\n",
    "\n",
    "Experiment with values between 0.01 to 0.05 for weight decay (AdamW).\n",
    "\n",
    "Gradually increase learning rate if necessary or use learning rate scheduling.\n",
    "\n",
    "Label Smoothing:\n",
    "\n",
    "Test 0.1 or 0.2 for label smoothing if the model is still overfitting. Alternatively, try no label smoothing and see how the test set accuracy behaves.\n",
    "\n",
    "Augmentation:\n",
    "\n",
    "Keep applying temporal augmentations but reduce their intensity (e.g., lower random flip probability or mild temporal jitter).\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "Consider reducing dropout or trying different dropout rates (0.3‚Äì0.5).\n",
    "\n",
    "Monitor Training with Early Stopping:\n",
    "\n",
    "Implement early stopping based on validation loss or test accuracy to prevent excessive overfitting.\n",
    "\n",
    "Experiment with Learning Rate Schedulers:\n",
    "\n",
    "Use StepLR or CosineAnnealingLR to adjust the learning rate dynamically.\n",
    "\n",
    "Example Code Snippet with Suggested Modifications:\n",
    "# CrossEntropy Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# AdamW optimizer with reduced weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "\n",
    "# Use scheduler for learning rate adjustments\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Early stopping (example using validation loss or accuracy)\n",
    "# Check if validation loss does not improve for 5 epochs and stop early\n",
    "\n",
    "Final Thoughts:\n",
    "\n",
    "Fine-tuning regularization parameters is often a matter of experimentation. Start with small changes and gradually fine-tune until you find the sweet spot.\n",
    "\n",
    "If after all these tweaks your model still doesn't generalize well, consider revisiting your data pipeline (e.g., more robust augmentation) or model architecture. Sometimes, even small changes in model architecture (like the number of layers, attention heads, or type of pooling) can lead to significant improvements in test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166734f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
